YTD Text Total = 
SUMX(
    FILTER(
        ALL('Date'),                 // Use ALL to remove any filters on Date
        'Date'[Date] <= MAX('Date'[Date])
    ),
    SUMX(
        FILTER(
            'Table2',
            'Table2'[Date] <= MAX('Date'[Date])
        ),
        VALUE('Table2'[TextColumn])   // Convert text to numeric and sum
    )
)


Count of Data Call Name running total in Fiscal Month Name 3 = 
CALCULATE(
	COUNTA('FY24DCT'[Data Call Name]),
	FILTER(
		CALCULATETABLE(
			SUMMARIZE(
				'DateTable1',
				'DateTable1'[Fiscal Month],
				'DateTable1'[Fiscal Month Name]
			),
			ALLSELECTED('DateTable1')
		),
		ISONORAFTER(
			'DateTable1'[Fiscal Month], MAX('DateTable1'[Fiscal Month]), DESC,
			'DateTable1'[Fiscal Month Name], MAX('DateTable1'[Fiscal Month Name]), DESC
		)
	)
)





import pandas as pd
from ydata_profiling import ProfileReport
import json

# Assuming 'data' contains the fetched JSON data with super columns

# Convert tuples to dictionaries if necessary
flattened_data = []
for item in data:
    # Extracting timestamp and dictionary from the tuple
    timestamp = item[0]
    dictionary_str = item[1]
    
    # Handling the case where the third element may be present
    if len(item) >= 3:
        third_element = item[2]
    else:
        third_element = None
    
    # Convert the dictionary string to a dictionary
    try:
        dictionary = json.loads(dictionary_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON string: {e}")
        continue
    
    # Flatten the dictionary and add timestamp
    flattened_item = {"timestamp": timestamp}
    for key, value in dictionary.items():
        flattened_item[key] = value
    
    # Add the third element if present
    flattened_item["third_element"] = third_element
    
    # Append to flattened_data
    flattened_data.append(flattened_item)

# Create a DataFrame from the flattened data
df = pd.DataFrame(flattened_data)

# Generate a ProfileReport
profile = ProfileReport(df, title="Test")

# Display the ProfileReport in the notebook
profile.to_notebook_iframe()

# Save the ProfileReport as an HTML file
profile.to_file("output.html")





import pandas as pd
import json
import pandas_profiling
import boto3
from redshift_connector import connect

# Define your AWS region
aws_region = 'your-aws-region'

# Create a Secrets Manager client
secrets_manager = boto3.client('secretsmanager', region_name=aws_region)

# Retrieve Redshift credentials from Secrets Manager
secret_name = 'your-redshift-secret-name'
response = secrets_manager.get_secret_value(SecretId=secret_name)
secret = json.loads(response['SecretString'])

# Extract Redshift connection parameters
redshift_host = secret['host']
redshift_db = secret['dbname']
redshift_user = secret['username']
redshift_password = secret['password']
redshift_port = secret['port']

# Construct the connection string
conn_str = f"redshift+psycopg2://{redshift_user}:{redshift_password}@{redshift_host}:{redshift_port}/{redshift_db}"

# Establish connection to Redshift
conn = connect(dsn=conn_str)

# Define your SQL query to fetch JSON data from the Redshift table
query = "SELECT * FROM your_table;"

# Load data into a Pandas DataFrame using the connection
df = pd.read_sql(query, conn)

# Now, let's explore the DataFrame and the JSON data

# Convert JSON strings to Python dictionaries for all JSON columns
for col in df.columns:
    if df[col].dtype == 'object':  # Assuming JSON columns have object data type
        df[col] = df[col].apply(json.loads)

# Generate data profile using pandas-profiling
profile = pandas_profiling.ProfileReport(df)

# Save the profile report to an HTML file
profile.to_file("data_profile_report.html")

# Optionally, you can also display the profile report in the notebook
# profile.to_widgets()

# Close the connection
conn.close()





Count of Data Call Name YTD 2 = 
TOTALYTD(COUNTA('FY24DCT'[Data Call Name]), 'DateTable1'[Date])


Count of Data Call Name YTD 2 =
CALCULATE(
    COUNTA('FY24DCT'[Data Call Name]),
    DATESYTD(
        'DateTable1'[Date],
        "9/30"  // Fiscal year end date (month/day format for September 30th)
    ),
    'DateTable1'[Date] <= TODAY()  // Ensures calculation up to today's date
)

(function process(/*RESTAPIRequest*/ request, /*RESTAPIResponse*/ response) {
	var oJSON = {};
	var rows = [];
	var na = [];
	
	
	try {		
		var row = {};
		var name = {};
        name.column_2 = "col 4 data";
		row.column_1 = "col 1 data";
		row.column_2 = "col 2 data";
		row.column_3 = "col 3 data";
		
		rows.push(row);	
		na.push(name);
		oJSON.status = 200;
		
	} catch(ex) {
		oJSON.error = ex.toString();
		oJSON.status = 400;
		
	} finally {
		response.setStatus(oJSON.status);
		oJSON.rows = rows;
		response.setBody(oJSON);
	}

})(request, response);



(function process(/*RESTAPIRequest*/ request, /*RESTAPIResponse*/ response) {

    var oJSON = {};
	var mfaStatuses = [];
	var mfa = new GlideRecord("x_g_usd2_ssn_priva_fisma_mfa");
	try {
		mfa.addQuery("approved", "Yes");
		mfa.addQuery("nothing_to_report", "False");
		mfa.addQuery("save_as_draft", "False");

		mfa.query();
		while (mfa.next()) {	
			var uid = mfa.uid.toString();
			if(uid == "FISMA-23-06" || uid == "FISMA-23-05") {
				var data = {};
				data.uid = uid;
				data.departmentalElement = mfa.departmental_element.getDisplayValue().toString();
				data.departmentalComponent = mfa.departmental_component.getDisplayValue().toString();
				
				data.mfaCompliant = mfa.question_6 ? mfa.question_6.toString() : "No data";
				data.ditCompliant = mfa.question_9 ? mfa.question_9.toString() : "No data";
				
				var hasSensitiveData = mfa.question_7 && mfa.question_7.toString() == "Yes";
				var darNotImplemented = mfa.question_8 && mfa.question_8.toString() == "No";
				if(hasSensitiveData && darNotImplemented) data.darCompliant = "No";
				else data.darCompliant = "Yes";
				
				mfaStatuses.push(data);
			}			
		}
		oJSON.status = 200;
	} catch(ex) {
		oJSON.error = ex.toString();
		oJSON.status = 400;
	} finally {
		response.setStatus(oJSON.status);
		oJSON.result = mfaStatuses;
		response.setBody(oJSON);
	}

})(request, response);


import psycopg2
import pandas as pd
from pandas_profiling import ProfileReport

# Connect to Redshift
conn = psycopg2.connect(
    dbname='your_dbname',
    user='your_username',
    password='your_password',
    host='your_redshift_host',
    port='your_redshift_port'
)

# Retrieve a sample of data from Redshift
query = "SELECT * FROM your_table LIMIT 100"
redshift_data = pd.read_sql(query, conn)

# Close the connection
conn.close()

# Perform data profiling
redshift_profile = ProfileReport(redshift_data, title='Redshift Data Profiling')

# Display the profiling results
redshift_profile.to_file("redshift_data_profile.html")



import pandas as pd
import boto3
from io import StringIO
from pandas_profiling import ProfileReport

# Connect to S3
s3 = boto3.client('s3',
                  aws_access_key_id='your_access_key_id',
                  aws_secret_access_key='your_secret_access_key')

# Specify the bucket and object key of your S3 data
bucket_name = 'your_bucket_name'
object_key = 'your_object_key'

# Retrieve a sample of data from S3
s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)
s3_data = pd.read_csv(s3_object['Body'], nrows=100)

# Perform data profiling
s3_profile = ProfileReport(s3_data, title='S3 Data Profiling')

# Display the profiling results
s3_profile.to_file("s3_data_profile.html")








import pandas as pd
import json
import pandas_profiling
from sqlalchemy import create_engine

# Define your Redshift connection parameters
redshift_host = 'your-redshift-host'
redshift_db = 'your-redshift-db'
redshift_user = 'your-redshift-username'
redshift_password = 'your-redshift-password'
redshift_port = 'your-redshift-port'

# Create the connection string for SQLAlchemy
connection_string = f'postgresql://{redshift_user}:{redshift_password}@{redshift_host}:{redshift_port}/{redshift_db}'

# Create SQLAlchemy engine
engine = create_engine(connection_string)

# Define your SQL query to fetch all JSON data columns from the Redshift table
query = "SELECT * FROM your_table;"

# Load data into a Pandas DataFrame using SQLAlchemy
with engine.connect() as conn:
    df = pd.read_sql_query(query, conn)

# Now, let's explore the DataFrame and the JSON data

# Identify columns containing JSON data
json_columns = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, (dict, list))).any()]

# Convert JSON strings to Python dictionaries for each JSON column
for col in json_columns:
    df[col] = df[col].apply(json.loads)

# Generate data profile using pandas-profiling
profile = pandas_profiling.ProfileReport(df)

# Save the profile report to an HTML file
profile.to_file("data_profile_report.html")

# Optionally, you can also display the profile report in the notebook
# profile.to_widgets()






import pandas as pd
from ydata_profiling import ProfileReport
import json

# Assuming 'data' contains the fetched JSON data with super columns

# Convert tuples to dictionaries if necessary
flattened_data = []
for item in data:
    # Extracting timestamp and dictionary from the tuple
    timestamp = item[0]
    dictionary_str = item[1]
    
    # Handling the case where the third element may be present
    if len(item) >= 3:
        third_element = item[2]
    else:
        third_element = None
    
    # Convert the dictionary string to a dictionary
    try:
        dictionary = json.loads(dictionary_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON string: {e}")
        continue
    
    # Flatten the dictionary and add timestamp
    flattened_item = {"timestamp": timestamp}
    for key, value in dictionary.items():
        flattened_item[key] = value
    
    # Add the third element if present
    flattened_item["third_element"] = third_element
    
    # Append to flattened_data
    flattened_data.append(flattened_item)

# Create a DataFrame from the flattened data
df = pd.DataFrame(flattened_data)

# Generate a ProfileReport
profile = ProfileReport(df, title="Test")

# Display the ProfileReport in the notebook
profile.to_notebook_iframe()

# Save the ProfileReport as an HTML file
profile.to_file("output.html")



Data quality checks in SQL can help ensure the integrity, accuracy, and consistency of data. Here are some common checks you can perform on two tables. Let's assume we have two tables: `TableA` and `TableB`.

### 1. Row Count Check
Ensure both tables have the expected number of rows.
```sql
-- Row count for TableA
SELECT COUNT(*) AS RowCount FROM TableA;

-- Row count for TableB
SELECT COUNT(*) AS RowCount FROM TableB;
```

### 2. Null Value Check
Check for null values in important columns.
```sql
-- Null check for important columns in TableA
SELECT COUNT(*) AS NullCount
FROM TableA
WHERE ImportantColumn IS NULL;

-- Null check for important columns in TableB
SELECT COUNT(*) AS NullCount
FROM TableB
WHERE ImportantColumn IS NULL;
```

### 3. Uniqueness Check
Ensure certain columns have unique values.
```sql
-- Uniqueness check for a column in TableA
SELECT ImportantColumn, COUNT(*)
FROM TableA
GROUP BY ImportantColumn
HAVING COUNT(*) > 1;

-- Uniqueness check for a column in TableB
SELECT ImportantColumn, COUNT(*)
FROM TableB
GROUP BY ImportantColumn
HAVING COUNT(*) > 1;
```

### 4. Referential Integrity Check
Ensure that foreign key relationships are maintained.
```sql
-- Check foreign key integrity from TableA to TableB
SELECT a.ForeignKeyColumn
FROM TableA a
LEFT JOIN TableB b ON a.ForeignKeyColumn = b.PrimaryKeyColumn
WHERE b.PrimaryKeyColumn IS NULL;
```

### 5. Data Type Check
Ensure columns have the expected data types.
```sql
-- Data type check (usually done through schema inspection, but here's a SQL way)
SELECT COLUMN_NAME, DATA_TYPE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_NAME = 'TableA';

SELECT COLUMN_NAME, DATA_TYPE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_NAME = 'TableB';
```

### 6. Range Check
Ensure numeric columns fall within expected ranges.
```sql
-- Range check for a numeric column in TableA
SELECT COUNT(*)
FROM TableA
WHERE NumericColumn < 0 OR NumericColumn > 100;

-- Range check for a numeric column in TableB
SELECT COUNT(*)
FROM TableB
WHERE NumericColumn < 0 OR NumericColumn > 100;
```

### 7. Date Consistency Check
Ensure dates are within reasonable bounds and follow expected chronological order.
```sql
-- Date consistency check for TableA
SELECT COUNT(*)
FROM TableA
WHERE DateColumn < '2000-01-01' OR DateColumn > GETDATE();

-- Date consistency check for TableB
SELECT COUNT(*)
FROM TableB
WHERE DateColumn < '2000-01-01' OR DateColumn > GETDATE();
```

### 8. Duplicate Check
Identify potential duplicate records based on certain columns.
```sql
-- Duplicate check for TableA
SELECT Column1, Column2, COUNT(*)
FROM TableA
GROUP BY Column1, Column2
HAVING COUNT(*) > 1;

-- Duplicate check for TableB
SELECT Column1, Column2, COUNT(*)
FROM TableB
GROUP BY Column1, Column2
HAVING COUNT(*) > 1;
```

### 9. Consistency Check Between Tables
Ensure that related data between the two tables is consistent.
```sql
-- Consistency check between TableA and TableB
SELECT a.Column1, b.Column2
FROM TableA a
JOIN TableB b ON a.ForeignKeyColumn = b.PrimaryKeyColumn
WHERE a.SomeColumn <> b.SomeColumn;
```

### 10. Value Distribution Check
Check the distribution of values in key columns to identify anomalies.
```sql
-- Value distribution check for a column in TableA
SELECT ImportantColumn, COUNT(*)
FROM TableA
GROUP BY ImportantColumn;

-- Value distribution check for a column in TableB
SELECT ImportantColumn, COUNT(*)
FROM TableB
GROUP BY ImportantColumn;
```

By running these checks regularly, you can maintain high data quality and ensure the integrity of your databases.



import psycopg2
import pandas as pd

# Define the Redshift connection parameters
redshift_params = {
    'dbname': 'your_database',
    'user': 'your_username',
    'password': 'your_password',
    'host': 'your_redshift_cluster_endpoint',
    'port': '5439'  # Default Redshift port
}

# Define the SQL queries to fetch data from Redshift
query_table_a = "SELECT * FROM TableA"
query_table_b = "SELECT * FROM TableB"

# Define a function to establish a connection to Redshift
def get_redshift_data(query, params):
    conn = psycopg2.connect(**params)
    try:
        df = pd.read_sql_query(query, conn)
    finally:
        conn.close()
    return df

# Retrieve data from Redshift
data_a = get_redshift_data(query_table_a, redshift_params)
data_b = get_redshift_data(query_table_b, redshift_params)

# Define a function to perform data quality checks
def data_quality_check(df, table_name):
    checks_passed = True
    checks = []

    # Check 1: Null values in the InsertTime column
    null_count = df['InsertTime'].isnull().sum()
    if null_count > 0:
        checks.append(f"{table_name}: Null values found in InsertTime column.")
        checks_passed = False

    # Check 2: Future dates in the InsertTime column
    future_dates = df[df['InsertTime'] > pd.Timestamp.now()].shape[0]
    if future_dates > 0:
        checks.append(f"{table_name}: Future dates found in InsertTime column.")
        checks_passed = False

    # Check 3: Historical dates check (e.g., dates before 2000-01-01)
    historical_dates = df[df['InsertTime'] < pd.Timestamp('2000-01-01')].shape[0]
    if historical_dates > 0:
        checks.append(f"{table_name}: Historical dates found in InsertTime column.")
        checks_passed = False

    # Add more checks as needed
    # ...

    # If no issues were found, add a pass message
    if checks_passed:
        checks.append(f"{table_name}: All checks passed.")

    return checks_passed, checks

# Perform data quality checks on both tables
status_a, messages_a = data_quality_check(data_a, "TableA")
status_b, messages_b = data_quality_check(data_b, "TableB")

# Print the results
print("\n".join(messages_a))
print("\n".join(messages_b))

# Overall status
if status_a and status_b:
    print("Overall Status: PASS")
else:
    print("Overall Status: FAIL")









def descriptive_stats(dataframe):
    # Number of reopenings
    reopenings = dataframe[dataframe['Event_Type'] == 'IP-Level Ticket Reopened']
    count_reopen = reopenings['Event_Type'].count()
    print("Total Number of Reopenings:", count_reopen)
    
    # Percentage of Reopenings
    reopening_pct = reopenings['unique_ip_ticket_id'].nunique() / dataframe['unique_ip_ticket_id'].nunique()
    print("% of Reopenings:", round(reopening_pct * 100, 2))
    
    # Mean
    days_mean = dataframe['Reopen_Deltas_int'].mean()
    print("Mean:", round(days_mean, 2))
    
    # Standard Deviation
    days_sd = dataframe['Reopen_Deltas_int'].std()
    print("SD:", round(days_sd, 2))
    
    # Median
    days_median = dataframe['Reopen_Deltas_int'].median()
    print("Median:", round(days_median, 2))
    
    # Min and Max
    reopen_min = dataframe['Reopen_Deltas_int'].min()
    reopen_max = dataframe['Reopen_Deltas_int'].max()
    print("Min:", round(reopen_min, 2))
    print("Max:", round(reopen_max, 2))
    
    # Quartiles
    lower_bound_pct = dataframe['Reopen_Deltas_int'].quantile(0.25)
    upper_bound_pct = dataframe['Reopen_Deltas_int'].quantile(0.75)
    print("25th Percentile:", round(lower_bound_pct, 2))
    print("75th Percentile:", round(upper_bound_pct, 2))
    
    # 5th and 95th Percentiles
    five_lower_bound_pct = dataframe['Reopen_Deltas_int'].quantile(0.05)
    ninety_five_upper_bound_pct = dataframe['Reopen_Deltas_int'].quantile(0.95)
    print("5th Percentile:", round(five_lower_bound_pct, 2))
    print("95th Percentile:", round(ninety_five_upper_bound_pct, 2))
    
    # Return values (if needed)
    return {
        "Total Number of Reopenings": count_reopen,
        "Percentage of Reopenings": round(reopening_pct * 100, 2),
        "Mean": round(days_mean, 2),
        "SD": round(days_sd, 2),
        "Median": round(days_median, 2),
        "Min": round(reopen_min, 2),
        "Max": round(reopen_max, 2),
        "25th Percentile": round(lower_bound_pct, 2),
        "75th Percentile": round(upper_bound_pct, 2),
        "5th Percentile": round(five_lower_bound_pct, 2),
        "95th Percentile": round(ninety_five_upper_bound_pct, 2)
    }

import pandas as pd

def descriptive_stats(dataframe, ticket_id):
    # Filter the dataframe for the specific ticket ID
    df_filtered = dataframe[dataframe['unique_ip_ticket_id'] == ticket_id]

    # Number of reopenings
    reopenings = df_filtered[df_filtered['Event_Type'] == 'IP-Level Ticket Reopened']
    count_reopen = reopenings['Event_Type'].count()
    
    # Percentage of Reopenings
    reopening_pct = reopenings['unique_ip_ticket_id'].nunique() / df_filtered['unique_ip_ticket_id'].nunique()
    
    # Mean
    days_mean = df_filtered['Reopen_Deltas_int'].mean()
    
    # Standard Deviation
    days_sd = df_filtered['Reopen_Deltas_int'].std()
    
    # Median
    days_median = df_filtered['Reopen_Deltas_int'].median()
    
    # Min and Max
    reopen_min = df_filtered['Reopen_Deltas_int'].min()
    reopen_max = df_filtered['Reopen_Deltas_int'].max()
    
    # Quartiles
    lower_bound_pct = df_filtered['Reopen_Deltas_int'].quantile(0.25)
    upper_bound_pct = df_filtered['Reopen_Deltas_int'].quantile(0.75)
    
    # 5th and 95th Percentiles
    five_lower_bound_pct = df_filtered['Reopen_Deltas_int'].quantile(0.05)
    ninety_five_upper_bound_pct = df_filtered['Reopen_Deltas_int'].quantile(0.95)
    
    # Create a dictionary to hold the results
    stats = {
        "Metric": [
            "Total Number of Reopenings", 
            "Percentage of Reopenings", 
            "Mean", 
            "SD", 
            "Median", 
            "Min", 
            "Max", 
            "25th Percentile", 
            "75th Percentile", 
            "5th Percentile", 
            "95th Percentile"
        ],
        "Value": [
            count_reopen,
            round(reopening_pct * 100, 2),
            round(days_mean, 2),
            round(days_sd, 2),
            round(days_median, 2),
            round(reopen_min, 2),
            round(reopen_max, 2),
            round(lower_bound_pct, 2),
            round(upper_bound_pct, 2),
            round(five_lower_bound_pct, 2),
            round(ninety_five_upper_bound_pct, 2)
        ]
    }
    
    # Convert the dictionary to a DataFrame
    stats_df = pd.DataFrame(stats)
    
    return stats_df


result_df = descriptive_stats(your_dataframe, 'your_ticket_id')
print(result_df)
