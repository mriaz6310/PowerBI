YTD Text Total = 
SUMX(
    FILTER(
        ALL('Date'),                 // Use ALL to remove any filters on Date
        'Date'[Date] <= MAX('Date'[Date])
    ),
    SUMX(
        FILTER(
            'Table2',
            'Table2'[Date] <= MAX('Date'[Date])
        ),
        VALUE('Table2'[TextColumn])   // Convert text to numeric and sum
    )
)


Count of Data Call Name running total in Fiscal Month Name 3 = 
CALCULATE(
	COUNTA('FY24DCT'[Data Call Name]),
	FILTER(
		CALCULATETABLE(
			SUMMARIZE(
				'DateTable1',
				'DateTable1'[Fiscal Month],
				'DateTable1'[Fiscal Month Name]
			),
			ALLSELECTED('DateTable1')
		),
		ISONORAFTER(
			'DateTable1'[Fiscal Month], MAX('DateTable1'[Fiscal Month]), DESC,
			'DateTable1'[Fiscal Month Name], MAX('DateTable1'[Fiscal Month Name]), DESC
		)
	)
)






import pandas as pd
import json
import pandas_profiling
import boto3
from redshift_connector import connect

# Define your AWS region
aws_region = 'your-aws-region'

# Create a Secrets Manager client
secrets_manager = boto3.client('secretsmanager', region_name=aws_region)

# Retrieve Redshift credentials from Secrets Manager
secret_name = 'your-redshift-secret-name'
response = secrets_manager.get_secret_value(SecretId=secret_name)
secret = json.loads(response['SecretString'])

# Extract Redshift connection parameters
redshift_host = secret['host']
redshift_db = secret['dbname']
redshift_user = secret['username']
redshift_password = secret['password']
redshift_port = secret['port']

# Construct the connection string
conn_str = f"redshift+psycopg2://{redshift_user}:{redshift_password}@{redshift_host}:{redshift_port}/{redshift_db}"

# Establish connection to Redshift
conn = connect(dsn=conn_str)

# Define your SQL query to fetch JSON data from the Redshift table
query = "SELECT * FROM your_table;"

# Load data into a Pandas DataFrame using the connection
df = pd.read_sql(query, conn)

# Now, let's explore the DataFrame and the JSON data

# Convert JSON strings to Python dictionaries for all JSON columns
for col in df.columns:
    if df[col].dtype == 'object':  # Assuming JSON columns have object data type
        df[col] = df[col].apply(json.loads)

# Generate data profile using pandas-profiling
profile = pandas_profiling.ProfileReport(df)

# Save the profile report to an HTML file
profile.to_file("data_profile_report.html")

# Optionally, you can also display the profile report in the notebook
# profile.to_widgets()

# Close the connection
conn.close()





Count of Data Call Name YTD 2 = 
TOTALYTD(COUNTA('FY24DCT'[Data Call Name]), 'DateTable1'[Date])


Count of Data Call Name YTD 2 =
CALCULATE(
    COUNTA('FY24DCT'[Data Call Name]),
    DATESYTD(
        'DateTable1'[Date],
        "9/30"  // Fiscal year end date (month/day format for September 30th)
    ),
    'DateTable1'[Date] <= TODAY()  // Ensures calculation up to today's date
)

(function process(/*RESTAPIRequest*/ request, /*RESTAPIResponse*/ response) {
	var oJSON = {};
	var rows = [];
	var na = [];
	
	
	try {		
		var row = {};
		var name = {};
        name.column_2 = "col 4 data";
		row.column_1 = "col 1 data";
		row.column_2 = "col 2 data";
		row.column_3 = "col 3 data";
		
		rows.push(row);	
		na.push(name);
		oJSON.status = 200;
		
	} catch(ex) {
		oJSON.error = ex.toString();
		oJSON.status = 400;
		
	} finally {
		response.setStatus(oJSON.status);
		oJSON.rows = rows;
		response.setBody(oJSON);
	}

})(request, response);



(function process(/*RESTAPIRequest*/ request, /*RESTAPIResponse*/ response) {

    var oJSON = {};
	var mfaStatuses = [];
	var mfa = new GlideRecord("x_g_usd2_ssn_priva_fisma_mfa");
	try {
		mfa.addQuery("approved", "Yes");
		mfa.addQuery("nothing_to_report", "False");
		mfa.addQuery("save_as_draft", "False");

		mfa.query();
		while (mfa.next()) {	
			var uid = mfa.uid.toString();
			if(uid == "FISMA-23-06" || uid == "FISMA-23-05") {
				var data = {};
				data.uid = uid;
				data.departmentalElement = mfa.departmental_element.getDisplayValue().toString();
				data.departmentalComponent = mfa.departmental_component.getDisplayValue().toString();
				
				data.mfaCompliant = mfa.question_6 ? mfa.question_6.toString() : "No data";
				data.ditCompliant = mfa.question_9 ? mfa.question_9.toString() : "No data";
				
				var hasSensitiveData = mfa.question_7 && mfa.question_7.toString() == "Yes";
				var darNotImplemented = mfa.question_8 && mfa.question_8.toString() == "No";
				if(hasSensitiveData && darNotImplemented) data.darCompliant = "No";
				else data.darCompliant = "Yes";
				
				mfaStatuses.push(data);
			}			
		}
		oJSON.status = 200;
	} catch(ex) {
		oJSON.error = ex.toString();
		oJSON.status = 400;
	} finally {
		response.setStatus(oJSON.status);
		oJSON.result = mfaStatuses;
		response.setBody(oJSON);
	}

})(request, response);


import psycopg2
import pandas as pd
from pandas_profiling import ProfileReport

# Connect to Redshift
conn = psycopg2.connect(
    dbname='your_dbname',
    user='your_username',
    password='your_password',
    host='your_redshift_host',
    port='your_redshift_port'
)

# Retrieve a sample of data from Redshift
query = "SELECT * FROM your_table LIMIT 100"
redshift_data = pd.read_sql(query, conn)

# Close the connection
conn.close()

# Perform data profiling
redshift_profile = ProfileReport(redshift_data, title='Redshift Data Profiling')

# Display the profiling results
redshift_profile.to_file("redshift_data_profile.html")



import pandas as pd
import boto3
from io import StringIO
from pandas_profiling import ProfileReport

# Connect to S3
s3 = boto3.client('s3',
                  aws_access_key_id='your_access_key_id',
                  aws_secret_access_key='your_secret_access_key')

# Specify the bucket and object key of your S3 data
bucket_name = 'your_bucket_name'
object_key = 'your_object_key'

# Retrieve a sample of data from S3
s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)
s3_data = pd.read_csv(s3_object['Body'], nrows=100)

# Perform data profiling
s3_profile = ProfileReport(s3_data, title='S3 Data Profiling')

# Display the profiling results
s3_profile.to_file("s3_data_profile.html")








import pandas as pd
import json
import pandas_profiling
from sqlalchemy import create_engine

# Define your Redshift connection parameters
redshift_host = 'your-redshift-host'
redshift_db = 'your-redshift-db'
redshift_user = 'your-redshift-username'
redshift_password = 'your-redshift-password'
redshift_port = 'your-redshift-port'

# Create the connection string for SQLAlchemy
connection_string = f'postgresql://{redshift_user}:{redshift_password}@{redshift_host}:{redshift_port}/{redshift_db}'

# Create SQLAlchemy engine
engine = create_engine(connection_string)

# Define your SQL query to fetch all JSON data columns from the Redshift table
query = "SELECT * FROM your_table;"

# Load data into a Pandas DataFrame using SQLAlchemy
with engine.connect() as conn:
    df = pd.read_sql_query(query, conn)

# Now, let's explore the DataFrame and the JSON data

# Identify columns containing JSON data
json_columns = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, (dict, list))).any()]

# Convert JSON strings to Python dictionaries for each JSON column
for col in json_columns:
    df[col] = df[col].apply(json.loads)

# Generate data profile using pandas-profiling
profile = pandas_profiling.ProfileReport(df)

# Save the profile report to an HTML file
profile.to_file("data_profile_report.html")

# Optionally, you can also display the profile report in the notebook
# profile.to_widgets()






import pandas as pd
from ydata_profiling import ProfileReport
import json

# Assuming 'data' contains the fetched JSON data with super columns

# Convert tuples to dictionaries if necessary
flattened_data = []
for item in data:
    if isinstance(item, dict):
        flattened_item = {}
        for key, value in item.items():
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    flattened_item[f"{key}_{sub_key}"] = sub_value
            else:
                flattened_item[key] = value
        flattened_data.append(flattened_item)
    elif isinstance(item, tuple):
        try:
            flattened_data.append(dict(item))
        except ValueError as e:
            print(f"Error converting tuple to dictionary: {e}")
    else:
        print("Unexpected data format. Cannot flatten.")

# Create a DataFrame from the flattened data
df = pd.DataFrame(flattened_data)

# Generate a ProfileReport
profile = ProfileReport(df, title="Test")

# Display the ProfileReport in the notebook
profile.to_notebook_iframe()

# Save the ProfileReport as an HTML file
profile.to_file("output.html")
