SELECT 
  'GRANT "' || pg_get_userbyid(roleid) || '" TO "' || pg_get_userbyid(member) || '";'
FROM pg_auth_members
WHERE pg_get_userbyid(roleid) NOT IN ('postgres', 'rdsadmin', 'rdsproxy')
  AND pg_get_userbyid(member) NOT IN ('postgres', 'rdsadmin', 'rdsproxy')
ORDER BY 1;


SELECT 
  'GRANT ' || privilege_type || ' ON "' || table_schema || '"."' || table_name || '" TO "' || grantee || '";'
FROM information_schema.role_table_grants
WHERE grantee NOT IN ('postgres', 'rdsadmin', 'rdsproxy')
ORDER BY grantee, table_schema, table_name;



SELECT 
  'GRANT USAGE ON SCHEMA "' || n.nspname || '" TO "' || r.rolname || '";'
FROM pg_roles r
CROSS JOIN pg_namespace n
WHERE n.nspname NOT LIKE 'pg_%'
  AND n.nspname <> 'information_schema'
  AND r.rolname NOT IN ('postgres', 'rdsadmin', 'rdsproxy')
ORDER BY 1;


SELECT 
  n.nspname AS schema_name,
  'postgrestestadm' AS grantee,
  has_schema_privilege('postgrestestadm', n.nspname, 'USAGE') AS has_usage,
  has_schema_privilege('postgrestestadm', n.nspname, 'CREATE') AS has_create
FROM pg_namespace n
WHERE n.nspname NOT LIKE 'pg_%'
  AND n.nspname <> 'information_schema'
ORDER BY n.nspname;






BEGIN
  rdsadmin.rdsadmin_util.create_directory(
    p_directory_name => 'DATA_PUMP_DIR'
  );
END;
/

-- Then run your Data Pump job via PL/SQL or SQL Developer


BEGIN
  rdsadmin.rdsadmin_util.export_rds_datafile(
    p_directory_name => 'DATA_PUMP_DIR',
    p_prefix         => 'my_export',
    p_parallel       => 2,
    p_compression    => 'ALL'
  );
END;
/






DECLARE
  l_dp_handle NUMBER;
BEGIN
  -- Open export job (FULL export)
  l_dp_handle := DBMS_DATAPUMP.OPEN(
    operation => 'EXPORT',
    job_mode  => 'FULL',
    job_name  => 'MY_EXPORT_JOB'
  );

  -- Add dump file
  DBMS_DATAPUMP.ADD_FILE(
    handle    => l_dp_handle,
    filename  => 'my_export.dmp',
    directory => 'DATA_PUMP_DIR'
  );

  -- Add log file
  DBMS_DATAPUMP.ADD_FILE(
    handle    => l_dp_handle,
    filename  => 'my_export.log',
    directory => 'DATA_PUMP_DIR',
    filetype  => DBMS_DATAPUMP.KU$_FILE_TYPE_LOG_FILE
  );

  -- Start job
  DBMS_DATAPUMP.START_JOB(l_dp_handle);

  -- Detach so job runs in background
  DBMS_DATAPUMP.DETACH(l_dp_handle);
END;
/


SELECT * FROM TABLE(RDSADMIN.RDS_FILE_UTIL.LISTDIR('DATA_PUMP_DIR'));

SELECT text FROM TABLE(RDSADMIN.RDS_FILE_UTIL.READ_TEXT_FILE('DATA_PUMP_DIR','MY_EXPORT.LOG'));



BEGIN
  rdsadmin.rdsadmin_util.download_to_s3(
    p_directory_name => 'DATA_PUMP_DIR',
    p_s3_bucket      => 'your-s3-bucket-name',         -- Change this to your bucket
    p_s3_prefix      => 'oracle_exports/',             -- Optional "folder" path inside bucket
    p_file_name      => 'my_export.dmp'                 -- The dump file name you want to upload
  );
END;
/



BEGIN
  DBMS_CLOUD.CREATE_CREDENTIAL(
    credential_name => 'MY_S3_CRED',
    username        => 'YOUR_AWS_ACCESS_KEY_ID',
    password        => 'YOUR_AWS_SECRET_ACCESS_KEY'
  );
END;
/


BEGIN
  DBMS_CLOUD.PUT_OBJECT(
    credential_name => 'MY_S3_CRED',
    object_uri      => 'https://s3.amazonaws.com/your-s3-bucket-name/oracle_exports/my_export.dmp',
    directory_name  => 'DATA_PUMP_DIR',
    file_name       => 'my_export.dmp'
  );
END;
/

"s3:PutObject",
"s3:GetObject",
"s3:DeleteObject"



export ORACLE_HOME=/opt/oracle/instantclient
export PATH=$ORACLE_HOME:$PATH
export LD_LIBRARY_PATH=$ORACLE_HOME


expdp DEVDB_USER/PASSWORD@//source-rds-endpoint:1521/ORCL \
dumpfile=/data/oracle_dumps/my_export_%U.dmp \
logfile=/data/oracle_dumps/my_export.log \
full=y \
parallel=4




SELECT table_schema, table_name
FROM information_schema.tables
WHERE table_schema = 'sde'
  AND table_name = 'gdb_release'
  AND table_catalog = 'geogajh';





