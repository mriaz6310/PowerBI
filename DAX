YTD Text Total = 
SUMX(
    FILTER(
        ALL('Date'),                 // Use ALL to remove any filters on Date
        'Date'[Date] <= MAX('Date'[Date])
    ),
    SUMX(
        FILTER(
            'Table2',
            'Table2'[Date] <= MAX('Date'[Date])
        ),
        VALUE('Table2'[TextColumn])   // Convert text to numeric and sum
    )
)


Count of Data Call Name running total in Fiscal Month Name 3 = 
CALCULATE(
	COUNTA('FY24DCT'[Data Call Name]),
	FILTER(
		CALCULATETABLE(
			SUMMARIZE(
				'DateTable1',
				'DateTable1'[Fiscal Month],
				'DateTable1'[Fiscal Month Name]
			),
			ALLSELECTED('DateTable1')
		),
		ISONORAFTER(
			'DateTable1'[Fiscal Month], MAX('DateTable1'[Fiscal Month]), DESC,
			'DateTable1'[Fiscal Month Name], MAX('DateTable1'[Fiscal Month Name]), DESC
		)
	)
)

Count of Data Call Name YTD 2 = 
TOTALYTD(COUNTA('FY24DCT'[Data Call Name]), 'DateTable1'[Date])


Count of Data Call Name YTD 2 =
CALCULATE(
    COUNTA('FY24DCT'[Data Call Name]),
    DATESYTD(
        'DateTable1'[Date],
        "9/30"  // Fiscal year end date (month/day format for September 30th)
    ),
    'DateTable1'[Date] <= TODAY()  // Ensures calculation up to today's date
)

(function process(/*RESTAPIRequest*/ request, /*RESTAPIResponse*/ response) {
	var oJSON = {};
	var rows = [];
	var na = [];
	
	
	try {		
		var row = {};
		var name = {};
        name.column_2 = "col 4 data";
		row.column_1 = "col 1 data";
		row.column_2 = "col 2 data";
		row.column_3 = "col 3 data";
		
		rows.push(row);	
		na.push(name);
		oJSON.status = 200;
		
	} catch(ex) {
		oJSON.error = ex.toString();
		oJSON.status = 400;
		
	} finally {
		response.setStatus(oJSON.status);
		oJSON.rows = rows;
		response.setBody(oJSON);
	}

})(request, response);



(function process(/*RESTAPIRequest*/ request, /*RESTAPIResponse*/ response) {

    var oJSON = {};
	var mfaStatuses = [];
	var mfa = new GlideRecord("x_g_usd2_ssn_priva_fisma_mfa");
	try {
		mfa.addQuery("approved", "Yes");
		mfa.addQuery("nothing_to_report", "False");
		mfa.addQuery("save_as_draft", "False");

		mfa.query();
		while (mfa.next()) {	
			var uid = mfa.uid.toString();
			if(uid == "FISMA-23-06" || uid == "FISMA-23-05") {
				var data = {};
				data.uid = uid;
				data.departmentalElement = mfa.departmental_element.getDisplayValue().toString();
				data.departmentalComponent = mfa.departmental_component.getDisplayValue().toString();
				
				data.mfaCompliant = mfa.question_6 ? mfa.question_6.toString() : "No data";
				data.ditCompliant = mfa.question_9 ? mfa.question_9.toString() : "No data";
				
				var hasSensitiveData = mfa.question_7 && mfa.question_7.toString() == "Yes";
				var darNotImplemented = mfa.question_8 && mfa.question_8.toString() == "No";
				if(hasSensitiveData && darNotImplemented) data.darCompliant = "No";
				else data.darCompliant = "Yes";
				
				mfaStatuses.push(data);
			}			
		}
		oJSON.status = 200;
	} catch(ex) {
		oJSON.error = ex.toString();
		oJSON.status = 400;
	} finally {
		response.setStatus(oJSON.status);
		oJSON.result = mfaStatuses;
		response.setBody(oJSON);
	}

})(request, response);


import psycopg2
import pandas as pd
from pandas_profiling import ProfileReport

# Connect to Redshift
conn = psycopg2.connect(
    dbname='your_dbname',
    user='your_username',
    password='your_password',
    host='your_redshift_host',
    port='your_redshift_port'
)

# Retrieve a sample of data from Redshift
query = "SELECT * FROM your_table LIMIT 100"
redshift_data = pd.read_sql(query, conn)

# Close the connection
conn.close()

# Perform data profiling
redshift_profile = ProfileReport(redshift_data, title='Redshift Data Profiling')

# Display the profiling results
redshift_profile.to_file("redshift_data_profile.html")



import pandas as pd
import boto3
from io import StringIO
from pandas_profiling import ProfileReport

# Connect to S3
s3 = boto3.client('s3',
                  aws_access_key_id='your_access_key_id',
                  aws_secret_access_key='your_secret_access_key')

# Specify the bucket and object key of your S3 data
bucket_name = 'your_bucket_name'
object_key = 'your_object_key'

# Retrieve a sample of data from S3
s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)
s3_data = pd.read_csv(s3_object['Body'], nrows=100)

# Perform data profiling
s3_profile = ProfileReport(s3_data, title='S3 Data Profiling')

# Display the profiling results
s3_profile.to_file("s3_data_profile.html")
