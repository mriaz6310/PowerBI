import boto3
import pandas as pd
import os

# Replace these with your actual values
local_file_path = "C:/branch/DS/Xperia/your_file.xlsx"  # Update with the actual file name
s3_bucket_name = "your-s3-bucket-name"  # Replace with your S3 bucket name
s3_key = "path/in/bucket/your_file.xlsx"  # Specify the path and name in the S3 bucket

# Load the Excel file as a pandas DataFrame
df = pd.read_excel(local_file_path)

# Save the DataFrame as a temporary CSV file for upload
temp_csv = "/tmp/temp_data.csv"
df.to_csv(temp_csv, index=False)

# Upload the CSV to S3
s3_client = boto3.client('s3')
try:
    s3_client.upload_file(temp_csv, s3_bucket_name, s3_key)
    print("File uploaded successfully!")
except Exception as e:
    print(f"An error occurred: {e}")

# Clean up temporary file
os.remove(temp_csv)




import boto3

# Define the SageMaker file path and S3 bucket details
sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Update with the actual file path in SageMaker
s3_bucket_name = "your-s3-bucket-name"  # Replace with your S3 bucket name
s3_key = "desired/path/in/bucket/datafile.xlsx"  # Specify the path and name in the S3 bucket

# Initialize the S3 client
s3_client = boto3.client('s3')

try:
    # Upload file to S3
    s3_client.upload_file(sagemaker_file_path, s3_bucket_name, s3_key)
    print("File uploaded successfully to S3!")
except Exception as e:
    print(f"An error occurred: {e}")




import os

sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Update to your actual file path

if os.path.exists(sagemaker_file_path):
    print("File found in SageMaker instance.")
else:
    print("File not found at the specified path.")



# Try uploading the test file to S3
try:
    s3_client.upload_file("/tmp/test_permission_check.txt", s3_bucket_name, test_s3_key)
    print("S3 upload succeeded. Permissions are in place.")
except s3_client.exceptions.ClientError as e:
    if e.response['Error']['Code'] == "AccessDenied":
        print("S3 upload failed: Access Denied. Check your IAM role permissions.")
    else:
        print(f"S3 upload failed with error: {e}")





import boto3

# Define the file path in SageMaker and the S3 URL
sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Replace with the actual file path in SageMaker
s3_url = "s3://your-s3-bucket-name/desired/path/in/bucket/datafile.xlsx"  # Replace with your S3 URL

# Parse the bucket and key from the S3 URL
s3_bucket_name = s3_url.split('/')[2]
s3_key = '/'.join(s3_url.split('/')[3:])

# Initialize the S3 client
s3_client = boto3.client('s3')

try:
    # Upload file to S3
    s3_client.upload_file(sagemaker_file_path, s3_bucket_name, s3_key)
    print("File uploaded successfully to S3!")
except Exception as e:
    print(f"An error occurred: {e}")





import os
import subprocess
from datetime import datetime

# Ensure all script paths are correct and files exist
missing_files = [script for script in scripts if not os.path.isfile(script)]
if missing_files:
    print(f"Error: The following files do not exist: {missing_files}")
    raise FileNotFoundError(f"Files not found: {missing_files}")
else:
    print("All script files exist and are ready for execution.")

# Function to run a Python script within a Conda environment
def run_python_script_with_conda(script_name, conda_bin_path, conda_env_name, log_directory):
    # Define a log file for each individual script
    log_file = os.path.join(log_directory, f"{os.path.basename(script_name).replace('.py', '')}.log")
    
    try:
        print(f"Starting {script_name}...")
        # Build the bash command with proper paths
        bash_command = f'source "{conda_bin_path}" && conda activate {conda_env_name} && python "{script_name}"'
        
        # Run the Python script with Conda environment activation in a bash shell
        result = subprocess.run(
            ['bash', '-c', bash_command], 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True
        )
        
        # Write the script output to a log file
        with open(log_file, 'w') as log_output:
            log_output.write(result.stdout)
            log_output.write(result.stderr)
        
        # Print and return the result
        print(f"{script_name} finished with return code {result.returncode}")
        return result.returncode
    except Exception as e:
        print(f"Error running {script_name}: {e}")
        return None

# Append to master log file and execute scripts
with open("master_log.log", "a") as master_log:
    master_log.write("Script Name | Status       | Start Time           | End Time             | Completion Time (seconds) | Completion Timestamp\n")
    master_log.write("_" * 120 + "\n")

    # Run scripts sequentially
    for i, script in enumerate(scripts[:8]):
        start_time = datetime.now()
        start_timestamp = start_time.strftime('%Y-%m-%d %H:%M:%S')
        
        try:
            # Log the start time
            print(f"Script {script} started at {start_timestamp}")
            
            # Run each Python script with Conda environment activation
            return_code = run_python_script_with_conda(script, conda_bin_path, conda_env_name, log_directory)
            end_time = datetime.now()
            end_timestamp = end_time.strftime('%Y-%m-%d %H:%M:%S')
            completion_time = (end_time - start_time).total_seconds()
            
            # Determine success or failure
            if return_code == 0:
                status = "Success"
            elif return_code is not None:
                status = f"Failed (Code: {return_code})"
            else:
                status = "Error Occurred"
            
            # Log result in master log with timestamps
            master_log.write(f"{os.path.basename(script)} | {status:<12} | {start_timestamp} | {end_timestamp} | {completion_time:.2f} seconds | {end_timestamp}\n")
            print(f"Logged result for {script} in master log with completion at {end_timestamp}.")
        
        except Exception as e:
            print(f"Error during execution of script {script}: {e}")
            error_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            master_log.write(f"{os.path.basename(script)} | Error Occurred | {start_timestamp} | N/A | N/A | {error_timestamp}\n")










# Ensure all script paths are correct and files exist
missing_files = [script for script in scripts if not os.path.isfile(script)]
if missing_files:
    print(f"Error: The following files do not exist: {missing_files}")
    raise FileNotFoundError(f"Files not found: {missing_files}")
else:
    print("All script files exist and are ready for execution.")

# Function to run a Python script within a Conda environment
def run_python_script_with_conda(script_name, conda_bin_path, conda_env_name):
    try:
        print(f"Starting {script_name}...")
        # Bash command to activate Conda environment and run the script
        bash_command = f'source "{conda_bin_path}" && conda activate {conda_env_name} && python "{script_name}"'

        # Run the command
        result = subprocess.run(
            ['bash', '-c', bash_command],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        # Check for log file existence in the log directory
        log_file = os.path.join(log_directory, f"{os.path.basename(script_name).replace('.py', '')}.log")
        if os.path.isfile(log_file):
            print(f"Existing log file found for {script_name}: {log_file}")
        else:
            # Write script output to a new log file if not already created
            with open(log_file, 'w') as log_output:
                log_output.write(result.stdout)
                log_output.write(result.stderr)

        print(f"{script_name} finished with return code {result.returncode}")
        return result.returncode
    except Exception as e:
        print(f"Error running {script_name}: {e}")
        return None

# Create a list to hold log data
log_data = []

# Run scripts sequentially and log details
for script in scripts:
    start_time = datetime.now()
    start_timestamp = start_time.strftime('%Y-%m-%d %H:%M:%S')

    try:
        # Run each Python script with Conda environment
        return_code = run_python_script_with_conda(script, conda_bin_path, conda_env_name)
        end_time = datetime.now()
        end_timestamp = end_time.strftime('%Y-%m-%d %H:%M:%S')
        completion_time = (end_time - start_time).total_seconds()

        # Determine status
        status = "Success" if return_code == 0 else f"Failed (Code: {return_code})"

        # Append log data
        log_data.append({
            "Script Name": os.path.basename(script),
            "Status": status,
            "Start Time": start_timestamp,
            "End Time": end_timestamp,
            "Completion Time (seconds)": completion_time
        })
        print(f"Logged result for {script} with completion at {end_timestamp}.")

    except Exception as e:
        print(f"Error during execution of {script}: {e}")
        error_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_data.append({
            "Script Name": os.path.basename(script),
            "Status": "Error Occurred",
            "Start Time": start_timestamp,
            "End Time": "N/A",
            "Completion Time (seconds)": "N/A"
        })

# Create a DataFrame from the log data
log_df = pd.DataFrame(log_data)

# Save the DataFrame to an Excel file
excel_file = "master_log.xlsx"
log_df.to_excel(excel_file, index=False)

# Upload the Excel file to S3
s3_client = boto3.client('s3')
bucket_name = 'your-s3-bucket-name'  # Replace with your bucket name
s3_key = f"logs/{excel_file}"  # Path in the S3 bucket

try:
    s3_client.upload_file(excel_file, bucket_name, s3_key)
    print(f"Successfully uploaded {excel_file} to s3://{bucket_name}/{s3_key}")
except Exception as e:
    print(f"Error uploading {excel_file} to S3: {e}")








