import boto3
import pandas as pd
import os

# Replace these with your actual values
local_file_path = "C:/branch/DS/Xperia/your_file.xlsx"  # Update with the actual file name
s3_bucket_name = "your-s3-bucket-name"  # Replace with your S3 bucket name
s3_key = "path/in/bucket/your_file.xlsx"  # Specify the path and name in the S3 bucket

# Load the Excel file as a pandas DataFrame
df = pd.read_excel(local_file_path)

# Save the DataFrame as a temporary CSV file for upload
temp_csv = "/tmp/temp_data.csv"
df.to_csv(temp_csv, index=False)

# Upload the CSV to S3
s3_client = boto3.client('s3')
try:
    s3_client.upload_file(temp_csv, s3_bucket_name, s3_key)
    print("File uploaded successfully!")
except Exception as e:
    print(f"An error occurred: {e}")

# Clean up temporary file
os.remove(temp_csv)




import boto3

# Define the SageMaker file path and S3 bucket details
sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Update with the actual file path in SageMaker
s3_bucket_name = "your-s3-bucket-name"  # Replace with your S3 bucket name
s3_key = "desired/path/in/bucket/datafile.xlsx"  # Specify the path and name in the S3 bucket

# Initialize the S3 client
s3_client = boto3.client('s3')

try:
    # Upload file to S3
    s3_client.upload_file(sagemaker_file_path, s3_bucket_name, s3_key)
    print("File uploaded successfully to S3!")
except Exception as e:
    print(f"An error occurred: {e}")




import os

sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Update to your actual file path

if os.path.exists(sagemaker_file_path):
    print("File found in SageMaker instance.")
else:
    print("File not found at the specified path.")



# Try uploading the test file to S3
try:
    s3_client.upload_file("/tmp/test_permission_check.txt", s3_bucket_name, test_s3_key)
    print("S3 upload succeeded. Permissions are in place.")
except s3_client.exceptions.ClientError as e:
    if e.response['Error']['Code'] == "AccessDenied":
        print("S3 upload failed: Access Denied. Check your IAM role permissions.")
    else:
        print(f"S3 upload failed with error: {e}")





import boto3

# Define the file path in SageMaker and the S3 URL
sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Replace with the actual file path in SageMaker
s3_url = "s3://your-s3-bucket-name/desired/path/in/bucket/datafile.xlsx"  # Replace with your S3 URL

# Parse the bucket and key from the S3 URL
s3_bucket_name = s3_url.split('/')[2]
s3_key = '/'.join(s3_url.split('/')[3:])

# Initialize the S3 client
s3_client = boto3.client('s3')

try:
    # Upload file to S3
    s3_client.upload_file(sagemaker_file_path, s3_bucket_name, s3_key)
    print("File uploaded successfully to S3!")
except Exception as e:
    print(f"An error occurred: {e}")





import os
import subprocess
from datetime import datetime

# Ensure all script paths are correct and files exist
missing_files = [script for script in scripts if not os.path.isfile(script)]
if missing_files:
    print(f"Error: The following files do not exist: {missing_files}")
    raise FileNotFoundError(f"Files not found: {missing_files}")
else:
    print("All script files exist and are ready for execution.")

# Function to run a Python script within a Conda environment
def run_python_script_with_conda(script_name, conda_bin_path, conda_env_name, log_directory):
    # Define a log file for each individual script
    log_file = os.path.join(log_directory, f"{os.path.basename(script_name).replace('.py', '')}.log")
    
    try:
        print(f"Starting {script_name}...")
        # Build the bash command with proper paths
        bash_command = f'source "{conda_bin_path}" && conda activate {conda_env_name} && python "{script_name}"'
        
        # Run the Python script with Conda environment activation in a bash shell
        result = subprocess.run(
            ['bash', '-c', bash_command], 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True
        )
        
        # Write the script output to a log file
        with open(log_file, 'w') as log_output:
            log_output.write(result.stdout)
            log_output.write(result.stderr)
        
        # Print and return the result
        print(f"{script_name} finished with return code {result.returncode}")
        return result.returncode
    except Exception as e:
        print(f"Error running {script_name}: {e}")
        return None

# Append to master log file and execute scripts
with open("master_log.log", "a") as master_log:
    master_log.write("Script Name | Status       | Start Time           | End Time             | Completion Time (seconds) | Completion Timestamp\n")
    master_log.write("_" * 120 + "\n")

    # Run scripts sequentially
    for i, script in enumerate(scripts[:8]):
        start_time = datetime.now()
        start_timestamp = start_time.strftime('%Y-%m-%d %H:%M:%S')
        
        try:
            # Log the start time
            print(f"Script {script} started at {start_timestamp}")
            
            # Run each Python script with Conda environment activation
            return_code = run_python_script_with_conda(script, conda_bin_path, conda_env_name, log_directory)
            end_time = datetime.now()
            end_timestamp = end_time.strftime('%Y-%m-%d %H:%M:%S')
            completion_time = (end_time - start_time).total_seconds()
            
            # Determine success or failure
            if return_code == 0:
                status = "Success"
            elif return_code is not None:
                status = f"Failed (Code: {return_code})"
            else:
                status = "Error Occurred"
            
            # Log result in master log with timestamps
            master_log.write(f"{os.path.basename(script)} | {status:<12} | {start_timestamp} | {end_timestamp} | {completion_time:.2f} seconds | {end_timestamp}\n")
            print(f"Logged result for {script} in master log with completion at {end_timestamp}.")
        
        except Exception as e:
            print(f"Error during execution of script {script}: {e}")
            error_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            master_log.write(f"{os.path.basename(script)} | Error Occurred | {start_timestamp} | N/A | N/A | {error_timestamp}\n")










# Ensure all script paths are correct and files exist
missing_files = [script for script in scripts if not os.path.isfile(script)]
if missing_files:
    print(f"Error: The following files do not exist: {missing_files}")
    raise FileNotFoundError(f"Files not found: {missing_files}")
else:
    print("All script files exist and are ready for execution.")

# Function to run a Python script within a Conda environment
def run_python_script_with_conda(script_name, conda_bin_path, conda_env_name):
    try:
        print(f"Starting {script_name}...")
        # Bash command to activate Conda environment and run the script
        bash_command = f'source "{conda_bin_path}" && conda activate {conda_env_name} && python "{script_name}"'

        # Run the command
        result = subprocess.run(
            ['bash', '-c', bash_command],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        # Check for log file existence in the log directory
        log_file = os.path.join(log_directory, f"{os.path.basename(script_name).replace('.py', '')}.log")
        if os.path.isfile(log_file):
            print(f"Existing log file found for {script_name}: {log_file}")
        else:
            # Write script output to a new log file if not already created
            with open(log_file, 'w') as log_output:
                log_output.write(result.stdout)
                log_output.write(result.stderr)

        print(f"{script_name} finished with return code {result.returncode}")
        return result.returncode
    except Exception as e:
        print(f"Error running {script_name}: {e}")
        return None

# Create a list to hold log data
log_data = []

# Run scripts sequentially and log details
for script in scripts:
    start_time = datetime.now()
    start_timestamp = start_time.strftime('%Y-%m-%d %H:%M:%S')

    try:
        # Run each Python script with Conda environment
        return_code = run_python_script_with_conda(script, conda_bin_path, conda_env_name)
        end_time = datetime.now()
        end_timestamp = end_time.strftime('%Y-%m-%d %H:%M:%S')
        completion_time = (end_time - start_time).total_seconds()

        # Determine status
        status = "Success" if return_code == 0 else f"Failed (Code: {return_code})"

        # Append log data
        log_data.append({
            "Script Name": os.path.basename(script),
            "Status": status,
            "Start Time": start_timestamp,
            "End Time": end_timestamp,
            "Completion Time (seconds)": completion_time
        })
        print(f"Logged result for {script} with completion at {end_timestamp}.")

    except Exception as e:
        print(f"Error during execution of {script}: {e}")
        error_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_data.append({
            "Script Name": os.path.basename(script),
            "Status": "Error Occurred",
            "Start Time": start_timestamp,
            "End Time": "N/A",
            "Completion Time (seconds)": "N/A"
        })

# Create a DataFrame from the log data
log_df = pd.DataFrame(log_data)

# Save the DataFrame to an Excel file
excel_file = "master_log.xlsx"
log_df.to_excel(excel_file, index=False)

# Upload the Excel file to S3
s3_client = boto3.client('s3')
bucket_name = 'your-s3-bucket-name'  # Replace with your bucket name
s3_key = f"logs/{excel_file}"  # Path in the S3 bucket

try:
    s3_client.upload_file(excel_file, bucket_name, s3_key)
    print(f"Successfully uploaded {excel_file} to s3://{bucket_name}/{s3_key}")
except Exception as e:
    print(f"Error uploading {excel_file} to S3: {e}")





import numpy as np

# Convert each timedelta object to seconds before applying mean, median, or average calculations
ip_ticket_reopen_df_list['ticketmean'] = ip_ticket_reopen_df_list['Reopen_Deltas'].apply(
    lambda x: np.mean([delta.total_seconds() for delta in x]) / (60 * 60 * 24)
)

ip_ticket_reopen_df_list['ticketmedian'] = ip_ticket_reopen_df_list['Reopen_Deltas'].apply(
    lambda x: np.median([delta.total_seconds() for delta in x]) / (60 * 60 * 24)
)

ip_ticket_reopen_df_list['ticketaverage'] = ip_ticket_reopen_df_list['Reopen_Deltas'].apply(
    lambda x: np.mean([delta.total_seconds() for delta in x]) / (60 * 60 * 24)
)

# Explode the DataFrame (if needed)
deltas_explode = ip_ticket_reopen_df_list.explode(['ticketmean', 'ticketmedian', 'ticketaverage'])

# Final DataFrame
print(deltas_explode)







SELECT usename, usecreatedb, usesuper, valuntil 
FROM pg_user;

SELECT grantee, privilege_type, table_schema, table_name 
FROM information_schema.role_table_grants 
WHERE grantee NOT IN ('rdsdb', 'awsuser');




SELECT 
    'UNLOAD (''' || 'SELECT * FROM "' || table_schema || '"."' || table_name || '"''' || ') 
    TO '''s3://your-bucket-name/' || table_schema || '/' || table_name || '_data_''' 
    IAM_ROLE '''arn:aws:iam::your-account-id:role/your-redshift-role''' 
    FORMAT AS PARQUET;'
FROM information_schema.tables 
WHERE table_schema = 'test';








Step 1: Extract Users and Their Roles from the Source Cluster

Run the following queries in Redshift Query Editor on the source cluster to get a list of users and their associated privileges.

1. Get List of Users
SELECT usename, usecreatedb, usesuper, valuntil 
FROM pg_user;
2. Get User Roles and Privileges
SELECT grantee, privilege_type, table_schema, table_name 
FROM information_schema.role_table_grants 
WHERE grantee NOT IN ('rdsdb', 'awsuser');
3. Get User Groups (if using Group Roles)
SELECT * FROM pg_group;
Save these outputs, as you’ll need them to recreate users, roles, and privileges on the new cluster.

Step 2: Generate CREATE USER Statements for the New Cluster

For each user, create a corresponding CREATE USER statement:

CREATE USER user_name WITH PASSWORD 'your-secure-password' 
[ CREATEUSER | NOCREATEUSER ] [ SUPERUSER | NOSUPERUSER ] 
[ VALID UNTIL 'infinity' ];
Use the extracted user attributes from Step 1.
Passwords are not extractable, so you'll need to reset them.
Step 3: Generate GRANT Statements for Table-Level Access

For each table privilege, generate the corresponding GRANT statement:

GRANT SELECT, INSERT, UPDATE, DELETE 
ON schema_name.table_name 
TO user_name;
If users were assigned to groups, use:

GRANT user_group TO user_name;
Step 4: Transfer Users and Permissions to the New Cluster

Connect to the new Redshift cluster.
Run the CREATE USER statements.
Execute the GRANT statements to restore user access.
Step 5: Transfer IAM Roles (if used for authentication)

If users authenticated using IAM roles, attach the same roles in the new cluster:

In AWS IAM Console, navigate to Roles.
Find and attach the IAM role used in the old Redshift cluster to the new cluster.
Run:
ALTER USER user_name 
ADD IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role';
Step 6: Validate User Migration

Run SELECT * FROM pg_user; on the new cluster to verify user creation.
Check SELECT * FROM information_schema.role_table_grants; to confirm access is restored.
Automation: Script to Generate SQL for Users and Grants
Run this in the source cluster to auto-generate SQL statements:

SELECT 'CREATE USER ' || usename || 
       ' WITH PASSWORD ''your-secure-password'' ' || 
       CASE WHEN usesuper THEN 'SUPERUSER' ELSE 'NOSUPERUSER' END || 
       ';'
FROM pg_user;
And for table privileges:

SELECT 'GRANT ' || privilege_type || 
       ' ON ' || table_schema || '.' || table_name || 
       ' TO ' || grantee || ';' 
FROM information_schema.role_table_grants;




GRANT USAGE ON SCHEMA your_schema TO your_user;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA your_schema TO your_user;
ALTER DEFAULT PRIVILEGES IN SCHEMA your_schema 
GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO your_user;



CREATE SCHEMA IF NOT EXISTS your_schema;

SELECT ddl 
FROM admin.v_generate_tbl_ddl
WHERE schemaname = 'your_schema';


SELECT 'CREATE TABLE "' || table_schema || '"."' || table_name || '" AS SELECT * FROM "' || table_schema || '"."' || table_name || '" LIMIT 0;'
FROM information_schema.tables
WHERE table_schema = 'your_schema';





SELECT 'TRUNCATE TABLE "' || table_schema || '"."' || table_name || '" RESTART IDENTITY;'
FROM information_schema.tables
WHERE table_schema = 'test';



SELECT 
    'UNLOAD (''SELECT * FROM "' || table_schema || '"."' || table_name || '"''') 
    TO ''s3://your-bucket-name/' || table_schema || '/' || table_name || '_data_'' 
    IAM_ROLE ''arn:aws:iam::your-account-id:role/your-redshift-role'' 
    FORMAT AS PARQUET 
    ALLOWOVERWRITE;'
FROM information_schema.tables 
WHERE table_schema = 'test';




SELECT 'CREATE TABLE sales.' || tablename || ' (' || 
       STRING_AGG(column || ' ' || type, ', ') || ');'
FROM (
    SELECT tablename, column, type, ordinal_position 
    FROM svv_columns 
    WHERE table_schema = 'sales'
    ORDER BY tablename, ordinal_position
) t
GROUP BY tablename;


SELECT 'CREATE TABLE analytics.' || tablename || ' (' || 
       STRING_AGG(column || ' ' || type, ', ') || ');'
FROM (
    SELECT tablename, column, type, ordinal_position 
    FROM svv_columns 
    WHERE table_schema = 'analytics'
    ORDER BY tablename, ordinal_position
) t
GROUP BY tablename;




SELECT 'CREATE TABLE ' || table_schema || '.' || table_name || ' (' || 
       LISTAGG(column_name || ' ' || data_type, ', ') 
       WITHIN GROUP (ORDER BY ordinal_position) || ');'
FROM svv_columns
WHERE table_schema = 'your_schema'
GROUP BY table_schema, table_name;


copy
pg_dump -U your_user -d your_database | gzip | aws s3 cp - s3://your-bucket-name/backup.sql.gz

restore
aws s3 cp s3://your-bucket-name/backup.sql.gz - | gunzip | psql -U your_user -d your_database



pg_dump -U postgres -h localhost -p 5432 -d sales_db | aws s3 cp - s3://my-s3-backups/sales_db_backup.sql


pg_dump -U postgres -h localhost -p 5432 -d sales_db --compress=9 | aws s3 cp - s3://my-s3-backups/sales_db_backup.sql.gz



parallel:
pg_dump -U postgres -h localhost -p 5432 -d sales_db --format=directory --jobs=4 -f sales_db_backup
aws s3 cp sales_db_backup s3://my-s3-backups/ --recursive


auto
pg_dump -U postgres -h localhost -p 5432 -d sales_db --format=custom | split -b 500M - sales_db_backup_part_

for file in sales_db_backup_part_*; do aws s3 cp "$file" s3://my-s3-backups/; done




pg_dump -U postgres -h localhost -p 5432 -d sales_db | Out-String | aws s3 cp - s3://my-s3-backups/sales_db_backup.sql

pg_dump -U postgres -h localhost -p 5432 -d sales_db --format=custom | aws s3 cp - s3://my-s3-backups/sales_db_backup.dump


pg_dump -U postgres -h localhost -p 5432 -d sales_db | gzip | aws s3 cp - s3://your-bucket-name/sales_db_backup.gz



Main Script to validate counts for all of the Dbs :

# Connection info
$hostname = "your-rds-endpoint.rds.amazonaws.com"
$port = 5432
$user = "your_user"
$env:PGPASSWORD = "your_password"

# List of databases (skip templates)
$databases = psql -U $user -h $hostname -p $port -d postgres -t -c "SELECT datname FROM pg_database WHERE datname NOT IN ('template0', 'template1', 'postgres');"

# Clean up and loop
foreach ($dbnameRaw in $databases) {
    $dbname = $dbnameRaw.Trim()
    Write-Host "`n===== Database: $dbname ====="

    # Get list of tables in public schema (modify schema if needed)
    $tables = psql -U $user -h $hostname -p $port -d $dbname -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_type = 'BASE TABLE';"

    foreach ($tableRaw in $tables) {
        $table = $tableRaw.Trim()
        if ($table -ne "") {
            $count = psql -U $user -h $hostname -p $port -d $dbname -t -c "SELECT COUNT(*) FROM public.\"$table\";"
            Write-Host "$dbname.$table => $($count.Trim()) rows"
        }
    }
}



Row Count Validation

SELECT COUNT(*) FROM your_table;

2. Checksum Validation (Data integrity)

SELECT MD5(string_agg(t::text, '')) FROM your_table t;

3. Sample Data Checks

-- First 10 rows
SELECT * FROM your_table ORDER BY primary_key_column LIMIT 10;

-- Specific known values (e.g., records you know were recently added)
SELECT * FROM your_table WHERE id = 'abc123';

4. Schema Validation

-- List columns and types
SELECT column_name, data_type, character_maximum_length
FROM information_schema.columns
WHERE table_name = 'your_table'
ORDER BY ordinal_position;


Full DDL

-- For full table DDL (if using pgAdmin or `pg_dump -s`)
pg_dump -s -t your_table your_db

5. Indexes, Constraints, and Foreign Keys
-- Indexes
SELECT indexname, indexdef FROM pg_indexes WHERE tablename = 'your_table';

-- Constraints

SELECT conname, contype FROM pg_constraint WHERE conrelid = 'your_table'::regclass;

-- Foreign Keys
SELECT conname, confrelid::regclass AS referenced_table
FROM pg_constraint
WHERE conrelid = 'your_table'::regclass AND contype = 'f';

6.Large Object / TOAST Data Check (if applicable)

SELECT LENGTH(bytea_column) FROM your_table ORDER BY LENGTH(bytea_column) DESC LIMIT 5;





SELECT datname AS database_name, pg_catalog.pg_get_userbyid(datdba) AS owner, pg_encoding_to_char(encoding) AS encoding, datcollate AS collation, datctype AS ctype, datistemplate AS is_template, datallowconn AS allow_connections, datconnlimit AS connection_limit, datfrozenxid, dattablespace, datacl FROM pg_database;

tar -cvf - /home/*user_abc | tee /c112-source/backups/all_user_abc.tar > /dev/null



Compress-Archive -Path "C:\Users" -DestinationPath "C:\Temp\UsersBackup.zip"

aws s3 cp "C:\Temp\UsersBackup.zip" s3://c100/usersdata/UsersBackup.zip

aws s3 ls s3://c100/usersdata/


Set-AWSCredential -AccessKey "AKIAEXAMPLEKEY" -SecretKey "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY" -StoreAs datasync-user

Write-S3Object -BucketName "c100" -Key "usersdata/UsersBackup.zip" -File "C:\Temp\UsersBackup.zip" -ProfileName "datasync-user"



Get-ChildItem -Path "C:\" -Recurse -Filter aws.exe -ErrorAction SilentlyContinue



# Set your PostgreSQL environment variables
$env:PGPASSWORD = "your_password_here"
$PGUSER = "masteradm"
$PGHOST = "hshehj-instance-1.hdhdhjehe.ad-g-west-1.rds.amazonaws.com"
$PGPORT = 5432
$PGDUMP_HOST = "ushdhehhd-instance-1.cyeurjneb.us-west-1.rds.amazonaws.com"
$PGDUMP_PORT = 5422

# Get list of databases excluding templates
$databases = psql -U $PGUSER -h $PGHOST -p $PGPORT -d postgres -t -c "SELECT datname FROM pg_database WHERE datname NOT IN ('template0', 'template1');"

foreach ($dbname in $databases) {
    $dbname = $dbname.Trim()
    $backupPath = "G:\home\folder\$dbname"

    # Create backup directory if it doesn't exist
    if (!(Test-Path $backupPath)) {
        New-Item -ItemType Directory -Path $backupPath | Out-Null
    }

    # Run pg_dump with parallel jobs
    pg_dump -U $PGUSER -h $PGDUMP_HOST -p $PGDUMP_PORT -F d -j 10 -d $dbname -f $backupPath
}


while ($true) {
    Write-Host "Still alive at $(Get-Date)"
    Start-Sleep -Seconds 300  # Sends a message every 5 minutes
}


foreach ($dbname in $databases) {
    $dbname = $dbname.Trim()
    $backupPath = "G:\home\folder\$dbname"

    if (!(Test-Path $backupPath)) {
        New-Item -ItemType Directory -Path $backupPath | Out-Null
    }

    Write-Host "Starting dump for $dbname at $(Get-Date)"
    
    pg_dump -U $PGUSER -h $PGDUMP_HOST -p $PGDUMP_PORT -F d -j 10 -d $dbname -f $backupPath

    Write-Host "Completed dump for $dbname at $(Get-Date)"
    Start-Sleep -Seconds 10  # Optional short pause
}



pg_dump -U user -h host -d mydb --schema-only -f schema_prev.sql

pg_dump -U user -h host -d mydb --schema-only -f schema_now.sql

diff schema_prev.sql schema_now.sql






# PostgreSQL credentials and server info
$env:PGPASSWORD = "YourPasswordHere"
$PGUSER = "YourUsername"
$PGHOST = "your.server.hostname"
$PGPORT = "5432"
$PG_BIN = "C:\Program Files\PostgreSQL\16\bin"  # Adjust to your installed version
$DUMP_ROOT = "C:\Pg_Dumps\AllDumps\ServerA"
$LOG_DIR = "C:\Pg_Dumps\AllDumps\logs\ServerA"

# Ensure log directory exists
if (-not (Test-Path -Path $LOG_DIR)) {
    New-Item -Path $LOG_DIR -ItemType Directory | Out-Null
}

# Loop through each subdirectory (each representing a DB)
$folders = Get-ChildItem -Path $DUMP_ROOT -Directory

foreach ($folder in $folders) {
    $DBNAME = $folder.Name
    $DUMP_PATH = $folder.FullName
    $LOG_PATH = Join-Path $LOG_DIR "$DBNAME-restore.log"

    Write-Host "`n=== Starting restore of [$DBNAME] from [$DUMP_PATH] ==="

    try {
        # Drop database if exists
        & "$PG_BIN\psql.exe" -h $PGHOST -p $PGPORT -U $PGUSER -d postgres `
            -c "DROP DATABASE IF EXISTS $DBNAME;" >> $LOG_PATH 2>&1

        # Recreate database
        & "$PG_BIN\psql.exe" -h $PGHOST -p $PGPORT -U $PGUSER -d postgres `
            -c "CREATE DATABASE $DBNAME;" >> $LOG_PATH 2>&1

        # Run pg_restore
        & "$PG_BIN\pg_restore.exe" `
            -h $PGHOST `
            -p $PGPORT `
            -U $PGUSER `
            -d $DBNAME `
            -Fd "$DUMP_PATH" `
            -j 8 `
            --no-owner `
            --verbose >> $LOG_PATH 2>&1

        Write-Host "✅ [$DBNAME] restore complete. Log: $LOG_PATH"
    }
    catch {
        Write-Host "❌ Restore failed for [$DBNAME]. Check log at $LOG_PATH"
    }
}



SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '$DBNAME' AND pid <> pg_backend_pid();



# PostgreSQL credentials and server info for ServerA
$env:PGPASSWORD = "YourPasswordHere"
$PGUSER = "YourUsername"
$PGHOST = "your.server.a.hostname"
$PGPORT = "5432"
$PG_BIN = "C:\Program Files\PostgreSQL\16\bin"  # Adjust if using a different version
$DUMP_ROOT = "C:\Pg_Dumps\AllDumps\ServerA"
$LOG_DIR = "C:\Pg_Dumps\AllDumps\logs\ServerA"

# Ensure the log directory exists
if (-not (Test-Path -Path $LOG_DIR)) {
    New-Item -Path $LOG_DIR -ItemType Directory | Out-Null
}

# Loop through each database subfolder (each is a directory-format pg_dump)
$folders = Get-ChildItem -Path $DUMP_ROOT -Directory

foreach ($folder in $folders) {
    $DBNAME = $folder.Name
    $DUMP_PATH = $folder.FullName
    $LOG_PATH = Join-Path $LOG_DIR "$DBNAME-restore.log"

    Write-Host "`n=== Starting restore for database [$DBNAME] ==="

    try {
        # Force terminate any active connections to the database
        & "$PG_BIN\psql.exe" -h $PGHOST -p $PGPORT -U $PGUSER -d postgres `
            -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '$DBNAME' AND pid <> pg_backend_pid();" >> $LOG_PATH 2>&1

        # Drop the database if it exists
        & "$PG_BIN\psql.exe" -h $PGHOST -p $PGPORT -U $PGUSER -d postgres `
            -c "DROP DATABASE IF EXISTS $DBNAME;" >> $LOG_PATH 2>&1

        # Recreate the database
        & "$PG_BIN\psql.exe" -h $PGHOST -p $PGPORT -U $PGUSER -d postgres `
            -c "CREATE DATABASE $DBNAME;" >> $LOG_PATH 2>&1

        # Run pg_restore from directory format using parallel jobs
        & "$PG_BIN\pg_restore.exe" `
            -h $PGHOST `
            -p $PGPORT `
            -U $PGUSER `
            -d $DBNAME `
            -Fd "$DUMP_PATH" `
            -j 8 `
            --no-owner `
            --verbose >> $LOG_PATH 2>&1

        Write-Host "✅ Restore complete for [$DBNAME]. Log: $LOG_PATH"
    }
    catch {
        Write-Host "❌ Restore failed for [$DBNAME]. Check log at $LOG_PATH"
    }
}









REM === Paths ===
set DUMP_ROOT=X:\Test_Hyra\esa
set LOGDIR=C:\Users\ahsan\Documents\pg_restore_logs

REM === Create Log Directory If Needed ===
if not exist "%LOGDIR%" mkdir "%LOGDIR%"

REM === List of DBs to restore (match folder names under DUMP_ROOT) ===
set DBS=db1 db2 db3 db4

for %%D in (%DBS%) do (
    set DBNAME=%%D
    set DUMP_PATH=%DUMP_ROOT%\!DBNAME!

    REM === Timestamp for logs ===
    for /f "tokens=1-4 delims=/ " %%a in ("%date%") do (
        set yyyy=%%d
        set mm=%%b
        set dd=%%c
    )
    for /f "tokens=1-2 delims=:." %%a in ("%time%") do (
        set hh=%%a
        set nn=%%b
    )
    set TIMESTAMP=%yyyy%-%mm%-%dd%_%hh%%nn%
    set OUTLOG=%LOGDIR%\!DBNAME!-restore-!TIMESTAMP!.out.log
    set ERRLOG=%LOGDIR%\!DBNAME!-restore-!TIMESTAMP!.err.log

    echo.
    echo === Restoring !DBNAME! ===
    echo Log: !OUTLOG!

    REM === Force Disconnect ===
    %PG_BIN%\psql.exe -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '!DBNAME!' AND pid <> pg_backend_pid();" >> "!OUTLOG!" 2>> "!ERRLOG!"

    REM === Drop & Recreate DB ===
    %PG_BIN%\psql.exe -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres -c "DROP DATABASE IF EXISTS \"!DBNAME!\";" >> "!OUTLOG!" 2>> "!ERRLOG!"
    %PG_BIN%\psql.exe -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres -c "CREATE DATABASE \"!DBNAME!\";" >> "!OUTLOG!" 2>> "!ERRLOG!"

    REM === Restore ===
    %PG_BIN%\pg_restore.exe -j 32 -h %PGHOST% -p %PGPORT% -U %PGUSER% -d "!DBNAME!" --no-owner -Fd "!DUMP_PATH!" --verbose >> "!OUTLOG!" 2>> "!ERRLOG!"

    echo ✅ Done restoring !DBNAME!
)

echo.
echo === All databases processed ===
exit /b 0






@echo off
setlocal EnableDelayedExpansion

REM === PostgreSQL Credentials ===
set PGPASSWORD=YourPasswordHere
set PGUSER=YourUsername
set PGHOST=your.rds.hostname.amazonaws.com
set PGPORT=5432
set PG_BIN=C:\Program Files\PostgreSQL\16\bin

REM === Directory where all database dump folders are located ===
set DUMP_ROOT=X:\Test_Hyra\esa

REM === Log Directory ===
set LOGDIR=C:\Users\ahsan\Documents\pg_restore_logs

REM === Ensure log directory exists ===
if not exist "%LOGDIR%" (
    mkdir "%LOGDIR%"
)

REM === Loop through each folder in the dump root ===
for /D %%D in ("%DUMP_ROOT%\*") do (
    set "DBNAME=%%~nxD"
    set "DUMP_PATH=%%D"

    REM === Generate timestamp for logs ===
    for /f "tokens=1-4 delims=/ " %%a in ("%date%") do (
        set yyyy=%%d
        set mm=%%b
        set dd=%%c
    )
    for /f "tokens=1-2 delims=:." %%a in ("%time%") do (
        set hh=%%a
        set nn=%%b
    )
    set "TIMESTAMP=!yyyy!-!mm!-!dd!_!hh!!nn!"
    set "OUTLOG=%LOGDIR%\!DBNAME!-restore-!TIMESTAMP!.out.log"
    set "ERRLOG=%LOGDIR%\!DBNAME!-restore-!TIMESTAMP!.err.log"

    echo.
    echo === Restoring database: !DBNAME! ===
    echo Log file: !OUTLOG!

    REM === Force disconnect active sessions ===
    "%PG_BIN%\psql.exe" -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres ^
    -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '!DBNAME!' AND pid <> pg_backend_pid();" >> "!OUTLOG!" 2>> "!ERRLOG!"

    REM === Drop and recreate the database ===
    "%PG_BIN%\psql.exe" -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres ^
    -c "DROP DATABASE IF EXISTS \"!DBNAME!\";" >> "!OUTLOG!" 2>> "!ERRLOG!"

    "%PG_BIN%\psql.exe" -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres ^
    -c "CREATE DATABASE \"!DBNAME!\";" >> "!OUTLOG!" 2>> "!ERRLOG!"

    REM === Restore from directory-format pg_dump ===
    "%PG_BIN%\pg_restore.exe" -j 32 -h %PGHOST% -p %PGPORT% -U %PGUSER% ^
    -d "!DBNAME!" --no-owner -Fd "!DUMP_PATH!" --verbose >> "!OUTLOG!" 2>> "!ERRLOG!"

    echo ✅ Done restoring !DBNAME!
)

echo.
echo === All databases restored successfully ===
exit /b 0




@echo off
setlocal EnableDelayedExpansion

REM === PostgreSQL Credentials ===
set PGPASSWORD=YourPasswordHere
set PGUSER=YourUsername
set PGHOST=your.rds.hostname.amazonaws.com
set PGPORT=5432
set PG_BIN=C:\Program Files\PostgreSQL\16\bin

REM === Set the DB name you want to restore (same name as dump folder) ===
set DBNAME=your_database_name_here

REM === Dump and log directories ===
set DUMP_ROOT=X:\Test_Hyra\esa
set DUMP_PATH=%DUMP_ROOT%\%DBNAME%
set LOGDIR=C:\Users\ahsan\Documents\pg_restore_logs

REM === Validate dump path ===
if not exist "%DUMP_PATH%" (
    echo ❌ ERROR: Dump folder "%DUMP_PATH%" not found.
    exit /b 1
)

REM === Create log folder if it doesn't exist ===
if not exist "%LOGDIR%" (
    mkdir "%LOGDIR%"
)

REM === Generate timestamp ===
for /f "tokens=1-4 delims=/ " %%a in ("%date%") do (
    set yyyy=%%d
    set mm=%%b
    set dd=%%c
)
for /f "tokens=1-2 delims=:." %%a in ("%time%") do (
    set hh=%%a
    set nn=%%b
)
set TIMESTAMP=%yyyy%-%mm%-%dd%_%hh%%nn%
set OUTLOG=%LOGDIR%\%DBNAME%-restore-%TIMESTAMP%.out.log
set ERRLOG=%LOGDIR%\%DBNAME%-restore-%TIMESTAMP%.err.log

echo.
echo === Starting restore for database [%DBNAME%]
echo Dump Path: %DUMP_PATH%
echo Log File: %OUTLOG%

REM === Force disconnect active sessions ===
"%PG_BIN%\psql.exe" -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres ^
-c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '%DBNAME%' AND pid <> pg_backend_pid();" >> "%OUTLOG%" 2>> "%ERRLOG%"

REM === Drop and recreate the database ===
"%PG_BIN%\psql.exe" -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres ^
-c "DROP DATABASE IF EXISTS \"%DBNAME%\";" >> "%OUTLOG%" 2>> "%ERRLOG%"

"%PG_BIN%\psql.exe" -h %PGHOST% -p %PGPORT% -U %PGUSER% -d postgres ^
-c "CREATE DATABASE \"%DBNAME%\";" >> "%OUTLOG%" 2>> "%ERRLOG%"

REM === Run restore ===
"%PG_BIN%\pg_restore.exe" -j 32 -h %PGHOST% -p %PGPORT% -U %PGUSER% ^
-d "%DBNAME%" --no-owner -Fd "%DUMP_PATH%" --verbose >> "%OUTLOG%" 2>> "%ERRLOG%"

echo ✅ Done restoring %DBNAME%
exit /b 0





& "C:\Program Files\PostgreSQL\16\bin\psql.exe" -U postgres -d postgres -c "SHOW config_file;"




SELECT 
  'CREATE ROLE ' || rolname ||
  CASE WHEN rolcanlogin THEN ' LOGIN' ELSE '' END ||
  CASE WHEN rolsuper THEN ' SUPERUSER' ELSE '' END ||
  CASE WHEN rolcreaterole THEN ' CREATEROLE' ELSE '' END ||
  CASE WHEN rolcreatedb THEN ' CREATEDB' ELSE '' END ||
  CASE WHEN rolinherit THEN ' INHERIT' ELSE '' END ||
  CASE WHEN rolreplication THEN ' REPLICATION' ELSE '' END ||
  CASE WHEN rolbypassrls THEN ' BYPASSRLS' ELSE '' END ||
  ' PASSWORD ' || quote_literal(rolpassword) || ';' AS create_role_sql
FROM pg_authid
WHERE rolname NOT IN ('rdsadmin', 'rdsproxy', 'postgres')  -- exclude AWS internal users or system users
  AND rolname <> current_user;


SELECT 
  'GRANT ' || privilege_type || ' ON ' || table_schema || '.' || table_name || ' TO ' || grantee || ';'
FROM information_schema.role_table_grants
WHERE grantee NOT IN ('postgres', 'rdsadmin', 'rdsproxy');


SELECT 
  'GRANT USAGE ON SCHEMA ' || quote_ident(n.nspname) || ' TO ' || quote_ident(r.rolname) || ';'
FROM 
  pg_roles r
CROSS JOIN 
  pg_namespace n
WHERE 
  n.nspname NOT LIKE 'pg_%' 
  AND n.nspname <> 'information_schema'
  AND r.rolname NOT IN ('postgres', 'rdsadmin');



SELECT 
  'CREATE ROLE "' || rolname || '"' ||
  CASE WHEN rolcanlogin THEN ' LOGIN' ELSE '' END ||
  CASE WHEN rolsuper THEN ' SUPERUSER' ELSE '' END ||
  CASE WHEN rolcreaterole THEN ' CREATEROLE' ELSE '' END ||
  CASE WHEN rolcreatedb THEN ' CREATEDB' ELSE '' END ||
  CASE WHEN rolinherit THEN ' INHERIT' ELSE '' END ||
  CASE WHEN rolreplication THEN ' REPLICATION' ELSE '' END ||
  CASE WHEN rolbypassrls THEN ' BYPASSRLS' ELSE '' END ||
  ';' AS create_role_sql
FROM pg_roles
WHERE rolname NOT IN ('postgres', 'rdsadmin', 'rdsproxy')
ORDER BY rolname;


SELECT 
    SUM(size) * 8 / 1024 AS TotalSizeMB,
    ROUND(SUM(size) * 8.0 / 1024 / 1024, 2) AS TotalSizeGB
FROM 
    sys.master_files
WHERE 
    type_desc IN ('ROWS', 'LOG');


