An A-B check for data quality is a comparison between two datasets to verify their consistency, accuracy, or to identify discrepancies. This is commonly used to validate data synchronization between two systems, data migrations, or to compare different stages in a data pipeline.

Let's consider two tables, `TableA` and `TableB`, where you want to ensure that:

1. Both tables have the same set of records (row-level consistency).
2. Specific columns in both tables contain matching values (column-level consistency).

Here are examples of SQL queries for performing A-B checks:

### 1. Row Count Check

Ensure both tables have the same number of rows.

```sql
-- Row count comparison
SELECT 
    (SELECT COUNT(*) FROM TableA) AS TableA_RowCount,
    (SELECT COUNT(*) FROM TableB) AS TableB_RowCount;
```

### 2. Primary Key Check

Ensure both tables have the same set of primary key values.

```sql
-- Assuming 'PrimaryKey' is the primary key column in both tables
-- Find keys in TableA but not in TableB
SELECT PrimaryKey
FROM TableA
WHERE PrimaryKey NOT IN (SELECT PrimaryKey FROM TableB);

-- Find keys in TableB but not in TableA
SELECT PrimaryKey
FROM TableB
WHERE PrimaryKey NOT IN (SELECT PrimaryKey FROM TableA);
```

### 3. Column Value Check

Compare the values of specific columns in both tables for matching primary keys.

```sql
-- Assuming columns 'Column1', 'Column2' need to be compared
-- Differences in 'Column1' for matching keys
SELECT a.PrimaryKey, a.Column1 AS TableA_Column1, b.Column1 AS TableB_Column1
FROM TableA a
JOIN TableB b ON a.PrimaryKey = b.PrimaryKey
WHERE a.Column1 <> b.Column1 OR a.Column1 IS NULL OR b.Column1 IS NULL;

-- Differences in 'Column2' for matching keys
SELECT a.PrimaryKey, a.Column2 AS TableA_Column2, b.Column2 AS TableB_Column2
FROM TableA a
JOIN TableB b ON a.PrimaryKey = b.PrimaryKey
WHERE a.Column2 <> b.Column2 OR a.Column2 IS NULL OR b.Column2 IS NULL;
```

### 4. Full Row Comparison

Ensure all columns match between the two tables for each primary key. This can be more complex depending on the number of columns but is essential for complete data validation.

```sql
-- Assuming the tables have columns Column1, Column2, Column3, etc.
-- Identify rows with any column mismatch
SELECT a.*
FROM TableA a
JOIN TableB b ON a.PrimaryKey = b.PrimaryKey
WHERE 
    (a.Column1 <> b.Column1 OR (a.Column1 IS NULL AND b.Column1 IS NOT NULL) OR (a.Column1 IS NOT NULL AND b.Column1 IS NULL))
    OR (a.Column2 <> b.Column2 OR (a.Column2 IS NULL AND b.Column2 IS NOT NULL) OR (a.Column2 IS NOT NULL AND b.Column2 IS NULL))
    OR (a.Column3 <> b.Column3 OR (a.Column3 IS NULL AND b.Column3 IS NOT NULL) OR (a.Column3 IS NOT NULL AND b.Column3 IS NULL));
```

### 5. Aggregate Value Check

Ensure that aggregate values (e.g., sums, averages) for specific columns match between the two tables.

```sql
-- Sum of 'Amount' column comparison
SELECT 
    (SELECT SUM(Amount) FROM TableA) AS TableA_AmountSum,
    (SELECT SUM(Amount) FROM TableB) AS TableB_AmountSum;

-- Average of 'Amount' column comparison
SELECT 
    (SELECT AVG(Amount) FROM TableA) AS TableA_AmountAvg,
    (SELECT AVG(Amount) FROM TableB) AS TableB_AmountAvg;
```

### 6. Date Range Check

Ensure both tables have records within the same date range.

```sql
-- Min and Max 'InsertTime' comparison
SELECT 
    (SELECT MIN(InsertTime) FROM TableA) AS TableA_MinDate,
    (SELECT MAX(InsertTime) FROM TableA) AS TableA_MaxDate,
    (SELECT MIN(InsertTime) FROM TableB) AS TableB_MinDate,
    (SELECT MAX(InsertTime) FROM TableB) AS TableB_MaxDate;
```

### 7. Missing Records Check

Identify records present in one table but missing in another based on primary key.

```sql
-- Records in TableA but not in TableB
SELECT a.*
FROM TableA a
LEFT JOIN TableB b ON a.PrimaryKey = b.PrimaryKey
WHERE b.PrimaryKey IS NULL;

-- Records in TableB but not in TableA
SELECT b.*
FROM TableB b
LEFT JOIN TableA a ON b.PrimaryKey = a.PrimaryKey
WHERE a.PrimaryKey IS NULL;
```

### 8. Count of Distinct Values

Compare the count of distinct values in specific columns between the two tables.

```sql
-- Count of distinct values in 'Category' column
SELECT 
    (SELECT COUNT(DISTINCT Category) FROM TableA) AS TableA_DistinctCategoryCount,
    (SELECT COUNT(DISTINCT Category) FROM TableB) AS TableB_DistinctCategoryCount;
```

### Example Scenario

Assume `TableA` is the source table and `TableB` is the destination table after a data migration. You want to ensure the data in `TableB` matches the original data in `TableA`. 

By running these SQL queries, you can systematically verify that:

- Both tables have the same number of records.
- All primary keys in `TableA` are present in `TableB`.
- Values in key columns are consistent between the two tables.
- There are no unexpected records missing from either table.

These checks help ensure data quality and integrity across different datasets.



SELECT 
    COLUMN_NAME, 
    DATA_TYPE, 
    CHARACTER_MAXIMUM_LENGTH, 
    IS_NULLABLE, 
    COLUMN_DEFAULT
FROM 
    INFORMATION_SCHEMA.COLUMNS
WHERE 
    TABLE_NAME = 'Orders' AND TABLE_SCHEMA = 'dbo';



SELECT 
    kc.column_name
FROM 
    information_schema.table_constraints AS tc
JOIN 
    information_schema.key_column_usage AS kc 
    ON tc.constraint_name = kc.constraint_name
    AND tc.table_schema = kc.table_schema
WHERE 
    tc.constraint_type = 'PRIMARY KEY'
    AND tc.table_name = 'your_table_name'
    AND tc.table_schema = 'your_schema_name';



import boto3
import json

# Initialize the S3 client
s3_client = boto3.client('s3')

# Replace with your bucket name
bucket_name = 'your-bucket-name'

# Replace with the prefix of your folders (if any)
prefix = 'your-prefix/'

# Get a list of all JSON files in the bucket
response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)

# Initialize an empty list to hold all data
all_data = []

# Iterate over the files
for obj in response.get('Contents', []):
    key = obj['Key']
    if key.endswith('.json'):
        # Download the JSON file
        file_content = s3_client.get_object(Bucket=bucket_name, Key=key)['Body'].read().decode('utf-8')
        
        # Load the JSON content
        json_data = json.loads(file_content)
        
        # Append the data to all_data list
        all_data.append(json_data)

# Write the combined data to a single file
with open('combined_data.json', 'w') as outfile:
    json.dump(all_data, outfile)

# Optionally, upload the combined file back to S3
s3_client.upload_file('combined_data.json', bucket_name, 'combined_data.json')

print("Combined JSON file created and uploaded successfully.")


# Upload the JSON file to S3
import boto3

s3_client = boto3.client('s3')
bucket_name = 'your-bucket-name'
json_file_path = 'path/to/your/large_json_file.json'
s3_key = 'your-folder/large_json_file.json'

s3_client.upload_file(json_file_path, bucket_name, s3_key)

# Load data into Redshift using the COPY command
conn = psycopg2.connect(
    dbname=redshift_db,
    user=redshift_user,
    password=redshift_password,
    host=redshift_endpoint,
    port=redshift_port
)
cur = conn.cursor()

copy_sql = """
COPY your_table_name (column1, column2, column3)
FROM 's3://your-bucket-name/your-folder/large_json_file.json'
IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role'
FORMAT AS JSON 'auto';
"""

# Execute the COPY command
cur.execute(copy_sql)
conn.commit()

# Close the connection
cur.close()
conn.close()



import json

# JSON schema definition
schema_json = '''
{
  "schema": "my_new_schema",
  "tables": [
    {
      "table_name": "my_new_table",
      "columns": [
        {"name": "column1", "type": "VARCHAR(255)"},
        {"name": "column2", "type": "INTEGER"},
        {"name": "column3", "type": "BOOLEAN"}
      ]
    }
  ]
}
'''

# Parse the JSON
schema = json.loads(schema_json)

# Generate SQL for creating schema
create_schema_sql = f"CREATE SCHEMA {schema['schema']};\n"

# Generate SQL for creating tables
create_table_sql = ""
for table in schema['tables']:
    table_name = table['table_name']
    columns = table['columns']
    columns_sql = ", ".join([f"{col['name']} {col['type']}" for col in columns])
    create_table_sql += f"CREATE TABLE {schema['schema']}.{table_name} ({columns_sql});\n"

# Combine the SQL statements
full_sql = create_schema_sql + create_table_sql
print(full_sql)


import boto3
import pandas as pd
import psycopg2
from io import StringIO

# Assuming df is your DataFrame
df = pd.json_normalize(data_list)

# Save DataFrame to CSV
csv_buffer = StringIO()
df.to_csv(csv_buffer, index=False)

# Upload CSV to S3
s3 = boto3.client('s3')
bucket_name = 'your-s3-bucket'
s3_key = 'your-s3-key/data.csv'
s3.put_object(Bucket=bucket_name, Key=s3_key, Body=csv_buffer.getvalue())

# Connect to Redshift
conn = psycopg2.connect(
    dbname='your_dbname',
    user='your_user',
    password='your_password',
    host='your_redshift_cluster_endpoint',
    port='5439'
)
cur = conn.cursor()

# Create table if it doesn't exist
create_table_query = """
CREATE TABLE IF NOT EXISTS your_table_name (
    column1 VARCHAR(255),
    column2 INTEGER,
    column3 BOOLEAN
    -- Add other columns as needed
);
"""
cur.execute(create_table_query)
conn.commit()

# Copy data from S3 to Redshift
copy_query = f"""
COPY your_table_name
FROM 's3://{bucket_name}/{s3_key}'
IAM_ROLE 'your_iam_role_arn'
CSV
IGNOREHEADER 1;
"""
cur.execute(copy_query)
conn.commit()

# Close the connection
cur.close()
conn.close()



