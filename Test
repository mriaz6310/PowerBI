import boto3
import pandas as pd
import os

# Replace these with your actual values
local_file_path = "C:/branch/DS/Xperia/your_file.xlsx"  # Update with the actual file name
s3_bucket_name = "your-s3-bucket-name"  # Replace with your S3 bucket name
s3_key = "path/in/bucket/your_file.xlsx"  # Specify the path and name in the S3 bucket

# Load the Excel file as a pandas DataFrame
df = pd.read_excel(local_file_path)

# Save the DataFrame as a temporary CSV file for upload
temp_csv = "/tmp/temp_data.csv"
df.to_csv(temp_csv, index=False)

# Upload the CSV to S3
s3_client = boto3.client('s3')
try:
    s3_client.upload_file(temp_csv, s3_bucket_name, s3_key)
    print("File uploaded successfully!")
except Exception as e:
    print(f"An error occurred: {e}")

# Clean up temporary file
os.remove(temp_csv)




import boto3

# Define the SageMaker file path and S3 bucket details
sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Update with the actual file path in SageMaker
s3_bucket_name = "your-s3-bucket-name"  # Replace with your S3 bucket name
s3_key = "desired/path/in/bucket/datafile.xlsx"  # Specify the path and name in the S3 bucket

# Initialize the S3 client
s3_client = boto3.client('s3')

try:
    # Upload file to S3
    s3_client.upload_file(sagemaker_file_path, s3_bucket_name, s3_key)
    print("File uploaded successfully to S3!")
except Exception as e:
    print(f"An error occurred: {e}")




import os

sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Update to your actual file path

if os.path.exists(sagemaker_file_path):
    print("File found in SageMaker instance.")
else:
    print("File not found at the specified path.")



# Try uploading the test file to S3
try:
    s3_client.upload_file("/tmp/test_permission_check.txt", s3_bucket_name, test_s3_key)
    print("S3 upload succeeded. Permissions are in place.")
except s3_client.exceptions.ClientError as e:
    if e.response['Error']['Code'] == "AccessDenied":
        print("S3 upload failed: Access Denied. Check your IAM role permissions.")
    else:
        print(f"S3 upload failed with error: {e}")





import boto3

# Define the file path in SageMaker and the S3 URL
sagemaker_file_path = "/path/to/your/datafile.xlsx"  # Replace with the actual file path in SageMaker
s3_url = "s3://your-s3-bucket-name/desired/path/in/bucket/datafile.xlsx"  # Replace with your S3 URL

# Parse the bucket and key from the S3 URL
s3_bucket_name = s3_url.split('/')[2]
s3_key = '/'.join(s3_url.split('/')[3:])

# Initialize the S3 client
s3_client = boto3.client('s3')

try:
    # Upload file to S3
    s3_client.upload_file(sagemaker_file_path, s3_bucket_name, s3_key)
    print("File uploaded successfully to S3!")
except Exception as e:
    print(f"An error occurred: {e}")





import os
import subprocess
from datetime import datetime

# Ensure all script paths are correct and files exist
missing_files = [script for script in scripts if not os.path.isfile(script)]
if missing_files:
    print(f"Error: The following files do not exist: {missing_files}")
    raise FileNotFoundError(f"Files not found: {missing_files}")
else:
    print("All script files exist and are ready for execution.")

# Function to run a Python script within a Conda environment
def run_python_script_with_conda(script_name, conda_bin_path, conda_env_name, log_directory):
    # Define a log file for each individual script
    log_file = os.path.join(log_directory, f"{os.path.basename(script_name).replace('.py', '')}.log")
    
    try:
        print(f"Starting {script_name}...")
        # Build the bash command with proper paths
        bash_command = f'source "{conda_bin_path}" && conda activate {conda_env_name} && python "{script_name}"'
        
        # Run the Python script with Conda environment activation in a bash shell
        result = subprocess.run(
            ['bash', '-c', bash_command], 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True
        )
        
        # Write the script output to a log file
        with open(log_file, 'w') as log_output:
            log_output.write(result.stdout)
            log_output.write(result.stderr)
        
        # Print and return the result
        print(f"{script_name} finished with return code {result.returncode}")
        return result.returncode
    except Exception as e:
        print(f"Error running {script_name}: {e}")
        return None

# Append to master log file and execute scripts
with open("master_log.log", "a") as master_log:
    master_log.write("Script Name | Status       | Start Time           | End Time             | Completion Time (seconds) | Completion Timestamp\n")
    master_log.write("_" * 120 + "\n")

    # Run scripts sequentially
    for i, script in enumerate(scripts[:8]):
        start_time = datetime.now()
        start_timestamp = start_time.strftime('%Y-%m-%d %H:%M:%S')
        
        try:
            # Log the start time
            print(f"Script {script} started at {start_timestamp}")
            
            # Run each Python script with Conda environment activation
            return_code = run_python_script_with_conda(script, conda_bin_path, conda_env_name, log_directory)
            end_time = datetime.now()
            end_timestamp = end_time.strftime('%Y-%m-%d %H:%M:%S')
            completion_time = (end_time - start_time).total_seconds()
            
            # Determine success or failure
            if return_code == 0:
                status = "Success"
            elif return_code is not None:
                status = f"Failed (Code: {return_code})"
            else:
                status = "Error Occurred"
            
            # Log result in master log with timestamps
            master_log.write(f"{os.path.basename(script)} | {status:<12} | {start_timestamp} | {end_timestamp} | {completion_time:.2f} seconds | {end_timestamp}\n")
            print(f"Logged result for {script} in master log with completion at {end_timestamp}.")
        
        except Exception as e:
            print(f"Error during execution of script {script}: {e}")
            error_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            master_log.write(f"{os.path.basename(script)} | Error Occurred | {start_timestamp} | N/A | N/A | {error_timestamp}\n")










# Ensure all script paths are correct and files exist
missing_files = [script for script in scripts if not os.path.isfile(script)]
if missing_files:
    print(f"Error: The following files do not exist: {missing_files}")
    raise FileNotFoundError(f"Files not found: {missing_files}")
else:
    print("All script files exist and are ready for execution.")

# Function to run a Python script within a Conda environment
def run_python_script_with_conda(script_name, conda_bin_path, conda_env_name):
    try:
        print(f"Starting {script_name}...")
        # Bash command to activate Conda environment and run the script
        bash_command = f'source "{conda_bin_path}" && conda activate {conda_env_name} && python "{script_name}"'

        # Run the command
        result = subprocess.run(
            ['bash', '-c', bash_command],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        # Check for log file existence in the log directory
        log_file = os.path.join(log_directory, f"{os.path.basename(script_name).replace('.py', '')}.log")
        if os.path.isfile(log_file):
            print(f"Existing log file found for {script_name}: {log_file}")
        else:
            # Write script output to a new log file if not already created
            with open(log_file, 'w') as log_output:
                log_output.write(result.stdout)
                log_output.write(result.stderr)

        print(f"{script_name} finished with return code {result.returncode}")
        return result.returncode
    except Exception as e:
        print(f"Error running {script_name}: {e}")
        return None

# Create a list to hold log data
log_data = []

# Run scripts sequentially and log details
for script in scripts:
    start_time = datetime.now()
    start_timestamp = start_time.strftime('%Y-%m-%d %H:%M:%S')

    try:
        # Run each Python script with Conda environment
        return_code = run_python_script_with_conda(script, conda_bin_path, conda_env_name)
        end_time = datetime.now()
        end_timestamp = end_time.strftime('%Y-%m-%d %H:%M:%S')
        completion_time = (end_time - start_time).total_seconds()

        # Determine status
        status = "Success" if return_code == 0 else f"Failed (Code: {return_code})"

        # Append log data
        log_data.append({
            "Script Name": os.path.basename(script),
            "Status": status,
            "Start Time": start_timestamp,
            "End Time": end_timestamp,
            "Completion Time (seconds)": completion_time
        })
        print(f"Logged result for {script} with completion at {end_timestamp}.")

    except Exception as e:
        print(f"Error during execution of {script}: {e}")
        error_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_data.append({
            "Script Name": os.path.basename(script),
            "Status": "Error Occurred",
            "Start Time": start_timestamp,
            "End Time": "N/A",
            "Completion Time (seconds)": "N/A"
        })

# Create a DataFrame from the log data
log_df = pd.DataFrame(log_data)

# Save the DataFrame to an Excel file
excel_file = "master_log.xlsx"
log_df.to_excel(excel_file, index=False)

# Upload the Excel file to S3
s3_client = boto3.client('s3')
bucket_name = 'your-s3-bucket-name'  # Replace with your bucket name
s3_key = f"logs/{excel_file}"  # Path in the S3 bucket

try:
    s3_client.upload_file(excel_file, bucket_name, s3_key)
    print(f"Successfully uploaded {excel_file} to s3://{bucket_name}/{s3_key}")
except Exception as e:
    print(f"Error uploading {excel_file} to S3: {e}")





import numpy as np

# Convert each timedelta object to seconds before applying mean, median, or average calculations
ip_ticket_reopen_df_list['ticketmean'] = ip_ticket_reopen_df_list['Reopen_Deltas'].apply(
    lambda x: np.mean([delta.total_seconds() for delta in x]) / (60 * 60 * 24)
)

ip_ticket_reopen_df_list['ticketmedian'] = ip_ticket_reopen_df_list['Reopen_Deltas'].apply(
    lambda x: np.median([delta.total_seconds() for delta in x]) / (60 * 60 * 24)
)

ip_ticket_reopen_df_list['ticketaverage'] = ip_ticket_reopen_df_list['Reopen_Deltas'].apply(
    lambda x: np.mean([delta.total_seconds() for delta in x]) / (60 * 60 * 24)
)

# Explode the DataFrame (if needed)
deltas_explode = ip_ticket_reopen_df_list.explode(['ticketmean', 'ticketmedian', 'ticketaverage'])

# Final DataFrame
print(deltas_explode)







SELECT usename, usecreatedb, usesuper, valuntil 
FROM pg_user;

SELECT grantee, privilege_type, table_schema, table_name 
FROM information_schema.role_table_grants 
WHERE grantee NOT IN ('rdsdb', 'awsuser');




SELECT 
    'UNLOAD (''' || 'SELECT * FROM "' || table_schema || '"."' || table_name || '"''' || ') 
    TO '''s3://your-bucket-name/' || table_schema || '/' || table_name || '_data_''' 
    IAM_ROLE '''arn:aws:iam::your-account-id:role/your-redshift-role''' 
    FORMAT AS PARQUET;'
FROM information_schema.tables 
WHERE table_schema = 'test';








Step 1: Extract Users and Their Roles from the Source Cluster

Run the following queries in Redshift Query Editor on the source cluster to get a list of users and their associated privileges.

1. Get List of Users
SELECT usename, usecreatedb, usesuper, valuntil 
FROM pg_user;
2. Get User Roles and Privileges
SELECT grantee, privilege_type, table_schema, table_name 
FROM information_schema.role_table_grants 
WHERE grantee NOT IN ('rdsdb', 'awsuser');
3. Get User Groups (if using Group Roles)
SELECT * FROM pg_group;
Save these outputs, as youâ€™ll need them to recreate users, roles, and privileges on the new cluster.

Step 2: Generate CREATE USER Statements for the New Cluster

For each user, create a corresponding CREATE USER statement:

CREATE USER user_name WITH PASSWORD 'your-secure-password' 
[ CREATEUSER | NOCREATEUSER ] [ SUPERUSER | NOSUPERUSER ] 
[ VALID UNTIL 'infinity' ];
Use the extracted user attributes from Step 1.
Passwords are not extractable, so you'll need to reset them.
Step 3: Generate GRANT Statements for Table-Level Access

For each table privilege, generate the corresponding GRANT statement:

GRANT SELECT, INSERT, UPDATE, DELETE 
ON schema_name.table_name 
TO user_name;
If users were assigned to groups, use:

GRANT user_group TO user_name;
Step 4: Transfer Users and Permissions to the New Cluster

Connect to the new Redshift cluster.
Run the CREATE USER statements.
Execute the GRANT statements to restore user access.
Step 5: Transfer IAM Roles (if used for authentication)

If users authenticated using IAM roles, attach the same roles in the new cluster:

In AWS IAM Console, navigate to Roles.
Find and attach the IAM role used in the old Redshift cluster to the new cluster.
Run:
ALTER USER user_name 
ADD IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role';
Step 6: Validate User Migration

Run SELECT * FROM pg_user; on the new cluster to verify user creation.
Check SELECT * FROM information_schema.role_table_grants; to confirm access is restored.
Automation: Script to Generate SQL for Users and Grants
Run this in the source cluster to auto-generate SQL statements:

SELECT 'CREATE USER ' || usename || 
       ' WITH PASSWORD ''your-secure-password'' ' || 
       CASE WHEN usesuper THEN 'SUPERUSER' ELSE 'NOSUPERUSER' END || 
       ';'
FROM pg_user;
And for table privileges:

SELECT 'GRANT ' || privilege_type || 
       ' ON ' || table_schema || '.' || table_name || 
       ' TO ' || grantee || ';' 
FROM information_schema.role_table_grants;




GRANT USAGE ON SCHEMA your_schema TO your_user;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA your_schema TO your_user;
ALTER DEFAULT PRIVILEGES IN SCHEMA your_schema 
GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO your_user;














